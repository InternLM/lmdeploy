name: daily_ete_test

on:
  push:
    branches:
      - hw_runner

env:
  REPORT_DIR: /test/test-reports/${{ github.run_id }}
  COV_PARAM: --cov /usr/local/python3.10.5/lib/python3.10/site-packages/lmdeploy
  FAIL_CONFIG: ${{ github.event_name == 'push' && github.run_attempt != 1 && '--lf --lfnf none' || '--lf'}}
  TEST_CODE_PATH: /test/lmdeploy_hw
  LOG_PATH: /test/log
  OFFLINE_REQUIREMENTS: /test/lmdeploy_hw/requirements_ascend.txt
  # Default values for former workflow_dispatch inputs
  BACKEND: '["turbomind", "pytorch"]'
  MODEL: '["llm","mllm"]'
  FUNCTION: '["pipeline", "restful", "chat"]'
  OFFLINE_MODE: false
  REGRESSION_FUNC: '["quant", "pipeline", "restful", "chat"]'
  TMPDIR: /mnt/deeplink/docker-tmp
  RAY_TMPDIR: /mnt/deeplink/docker-tmp

jobs:
  test_quantization:
    if: ${{ !cancelled() }}
    # if: ${{ !cancelled() && contains(fromJSON(env.REGRESSION_FUNC), 'quant') }}
    runs-on: [self-hosted, ascend-013]
    timeout-minutes: 150
    container:
      image: crpi-4crprmm5baj1v8iv.cn-hangzhou.personal.cr.aliyuncs.com/lmdeploy_dlinfer/ascend:910b-latest
      options: "--device=/dev/davinci0 --device=/dev/davinci1 --device=/dev/davinci2 --device=/dev/davinci3 --device=/dev/davinci4 --device=/dev/davinci5 --device=/dev/davinci6 --device=/dev/davinci7 --device=/dev/davinci_manager --device=/dev/devmm_svm --device=/dev/hisi_hdc -e PIP_CACHE_DIR=/root/.cache/pip --shm-size=150g --pull never"
      volumes:
        - /usr/local/Ascend/driver:/usr/local/Ascend/driver:ro
        - /usr/local/sbin:/usr/local/sbin:ro
        - /var/log/npu/slog:/var/log/npu/slog
        - /var/log/npu/profiling:/var/log/npu/profiling
        - /var/log/npu/dump:/var/log/npu/dump
        - /var/log/npu:/usr/slog
        - /etc/hccn.conf:/etc/hccn.conf:ro
        - /root/qa_test:/test
        - /mnt:/mnt
    steps:
      - name: Copy repository and Artifacts
        run: |
          cp -r ${{ env.TEST_CODE_PATH }}/. .
      - name: Install lmdeploy - dependency
        run: |
          python3 -m pip install -r ${{ env.OFFLINE_REQUIREMENTS }}
      - name: Install lmdeploy
        run: |
          python3 -m pip install -r requirements/test.txt
          python3 -m pip install transformers==4.53.1
      - name: Check env
        run: |
          python3 -m pip list
          lmdeploy check_env
          rm -rf allure-results
          # remove tmp log in testcase
          rm -rf ${{ env.LOG_PATH }}/*
          mkdir ${{ env.REPORT_DIR }}/.pytest_cache -p
          ln -s ${{ env.REPORT_DIR }}/.pytest_cache autotest
      - name: Test lmdeploy - quantization w4a16
        continue-on-error: true
        if: ${{ !cancelled() }}
        # if: ${{ contains(fromJSON(env.BACKEND), 'turbomind') }}
        run: |
          pytest autotest/tools/quantization/test_quantization_awq.py -m 'not pr_test' -n 8 --alluredir=${{ env.REPORT_DIR }} --clean-alluredir ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S')
      - name: Test lmdeploy - quantization w8a8
        continue-on-error: true
        if: ${{ !cancelled() }}
        # if: ${{ contains(fromJSON(env.BACKEND), 'pytorch') }}
        run: |
          pytest autotest/tools/quantization/test_quantization_w8a8.py -n 8 --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S')

  test_tools:
    if: ${{ !cancelled() }}
    # if: ${{ !cancelled() && contains(fromJSON(env.REGRESSION_FUNC), 'tools') }}
    runs-on: [self-hosted, ascend-013]
    needs: test_quantization
    timeout-minutes: 300
    strategy:
      fail-fast: false
      # matrix:
      #   backend: ${{ fromJSON(env.BACKEND) }}
      #   model: ${{ fromJSON(env.MODEL) }}
      #   function: ${{ fromJSON(env.FUNCTION) }}
      #   exclude:
      #     - backend: turbomind
      #       model: mllm
      #       function: chat
      #     - backend: pytorch
      #       model: mllm
      #       function: chat
      #   include:
      #     - backend: turbomind
      #       model: llm
      #       function: chat
    container:
      image: crpi-4crprmm5baj1v8iv.cn-hangzhou.personal.cr.aliyuncs.com/lmdeploy_dlinfer/ascend:910b-latest
      options: "--device=/dev/davinci0 --device=/dev/davinci1 --device=/dev/davinci2 --device=/dev/davinci3 --device=/dev/davinci4 --device=/dev/davinci5 --device=/dev/davinci6 --device=/dev/davinci7 --device=/dev/davinci_manager --device=/dev/devmm_svm --device=/dev/hisi_hdc -e PIP_CACHE_DIR=/root/.cache/pip --shm-size=150g --pull never"
      volumes:
        - /usr/local/Ascend/driver:/usr/local/Ascend/driver
        - /usr/local/sbin:/usr/local/sbin
        - /var/log/npu/slog:/var/log/npu/slog
        - /var/log/npu/profiling:/var/log/npu/profiling
        - /var/log/npu/dump:/var/log/npu/dump
        - /var/log/npu:/usr/slog
        - /etc/hccn.conf:/etc/hccn.conf
        - /root/qa_test:/test
        - /mnt:/mnt
    steps:
      - name: Copy repository and Artifacts
        run: |
          cp -r ${{ env.TEST_CODE_PATH }}/. .
      - name: Install lmdeploy - dependency
        run: |
          python3 -m pip install -r ${{ env.OFFLINE_REQUIREMENTS }}
      - name: Install lmdeploy
        run: |
          python3 -m pip install -r requirements/test.txt
      - name: Check env
        run: |
          python3 -m pip list
          lmdeploy check_env
          cp -r /root/lora .
          rm -rf allure-results
          # remove tmp log in testcase
          rm -rf ${{ env.LOG_PATH }}/*
          mkdir ${{ env.REPORT_DIR }}/.pytest_cache -p
          ln -s ${{ env.REPORT_DIR }}/.pytest_cache autotest
      - name: Test lmdeploy - chat
        continue-on-error: true
        if: ${{ !cancelled() }}
        # if: ${{ (matrix.backend == 'pytorch' || matrix.backend == 'turbomind') && matrix.model == 'llm' && matrix.function == 'chat' }}
        run: |
          pytest autotest/tools/chat/test_command_chat_hf_${{ matrix.backend }}.py -m 'gpu_num_1 and not pr_test' -n 8 --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S') || true
          pytest autotest/tools/chat/test_command_chat_hf_${{ matrix.backend }}.py -m 'gpu_num_2 and not pr_test' -n 4 --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S')
          pytest autotest/tools/chat/test_command_chat_hf_${{ matrix.backend }}.py -m 'gpu_num_4 and not pr_test' -n 2 --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S')
          pytest autotest/tools/chat/test_command_chat_hf_${{ matrix.backend }}.py -m 'gpu_num_8 and not pr_test' --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S')
      - name: Test lmdeploy - pipeline
        continue-on-error: true
        if: ${{ !cancelled() }}
        # if: ${{ matrix.function == 'pipeline' }}
        run: |
          pytest autotest/tools/pipeline/test_pipeline_chat_${{ matrix.backend }}_${{ matrix.model }}.py -m 'gpu_num_1 and not pr_test' -n 8 --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S') || true
          pytest autotest/tools/pipeline/test_pipeline_chat_${{ matrix.backend }}_${{ matrix.model }}.py -m 'gpu_num_2 and not pr_test' -n 4 --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S')
          pytest autotest/tools/pipeline/test_pipeline_chat_${{ matrix.backend }}_${{ matrix.model }}.py -m 'gpu_num_4 and not pr_test' -n 2 --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S')
          pytest autotest/tools/pipeline/test_pipeline_chat_${{ matrix.backend }}_${{ matrix.model }}.py -m 'gpu_num_8 and not pr_test' --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S')
      - name: Test lmdeploy - restful
        continue-on-error: true
        if: ${{ !cancelled() }}
        # if: ${{ matrix.function == 'restful' }}
        run: |
          pytest autotest/tools/restful/test_restful_chat_hf_${{ matrix.backend }}_${{ matrix.model }}.py -m 'gpu_num_1 and not pr_test' -n 8 --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S') || true
          pytest autotest/tools/restful/test_restful_chat_hf_${{ matrix.backend }}_${{ matrix.model }}.py -m 'gpu_num_2 and not pr_test' -n 4 --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S')
          pytest autotest/tools/restful/test_restful_chat_hf_${{ matrix.backend }}_${{ matrix.model }}.py -m 'gpu_num_4 and not pr_test' -n 2 --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S')
          pytest autotest/tools/restful/test_restful_chat_hf_${{ matrix.backend }}_${{ matrix.model }}.py -m 'gpu_num_8 and not pr_test' --alluredir=${{ env.REPORT_DIR }} ${{ env.COV_PARAM }} || true
          mv .coverage ${{ env.REPORT_DIR }}/.coverage.$(date +'%Y%m%d%H%M%S')