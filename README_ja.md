<div align="center">
  <img src="docs/en/_static/image/lmdeploy-logo.svg" width="450"/>

[![PyPI](https://img.shields.io/pypi/v/lmdeploy)](https://pypi.org/project/lmdeploy)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lmdeploy)
[![license](https://img.shields.io/github/license/InternLM/lmdeploy.svg)](https://github.com/InternLM/lmdeploy/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)
[![open issues](https://img.shields.io/github/issues-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)

[ğŸ“˜Documentation](https://lmdeploy.readthedocs.io/en/latest/) |
[ğŸ› ï¸Quick Start](https://lmdeploy.readthedocs.io/en/latest/get_started/get_started.html) |
[ğŸ¤”Reporting Issues](https://github.com/InternLM/lmdeploy/issues/new/choose)

[English](README.md) | [ç®€ä½“ä¸­æ–‡](README_zh-CN.md) | æ—¥æœ¬èª

ğŸ‘‹ join us on [![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=wechat&label=WeChat)](https://cdn.vansin.top/internlm/lmdeploy.jpg)
[![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=twitter&label=Twitter)](https://twitter.com/intern_lm)
[![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=discord&label=Discord)](https://discord.gg/xa29JuW87d)

</div>

______________________________________________________________________

## æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ ğŸ‰

<details close>
<summary><b>2024</b></summary>

- \[2024/08\] ğŸ”¥ğŸ”¥ LMDeployã¯[modelscope/swift](https://github.com/modelscope/swift)ã«çµ±åˆã•ã‚Œã€VLMsæ¨è«–ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ã¨ãªã‚Šã¾ã—ãŸ
- \[2024/07\] ğŸ‰ğŸ‰ Llama3.1 8Bã€70BãŠã‚ˆã³ãã®ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2024/07\] [InternVL2](https://huggingface.co/collections/OpenGVLab/internvl-20-667d3961ab5eb12c7ed1463e)å…¨ã‚·ãƒªãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ã€[InternLM-XComposer2.5](docs/en/multi_modal/xcomposer2d5.md)ãŠã‚ˆã³InternLM2.5ã®[ãƒ•ã‚¡ãƒ³ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ«](docs/en/llm/api_server_tools.md)ã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2024/06\] PyTorchã‚¨ãƒ³ã‚¸ãƒ³ã¯DeepSeek-V2ãŠã‚ˆã³ã„ãã¤ã‹ã®VLMsã€ä¾‹ãˆã°CogVLM2ã€Mini-InternVLã€LlaVA-Nextã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2024/05\] è¤‡æ•°ã®GPUã§VLMsã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹éš›ã«ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ©ãƒ³ã‚¹ã•ã›ã‚‹
- \[2024/05\] InternVL v1.5ã€LLaVaã€InternLMXComposer2ãªã©ã®VLMsã§4ãƒ“ãƒƒãƒˆã®é‡ã¿ã®ã¿ã®é‡å­åŒ–ã¨æ¨è«–ã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2024/04\] Llama3ãŠã‚ˆã³InternVL v1.1ã€v1.2ã€MiniGeminiã€InternLMXComposer2ãªã©ã®VLMãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2024/04\] TurboMindã¯ã™ã¹ã¦ã®ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã§ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³int8/int4 KVã‚­ãƒ£ãƒƒã‚·ãƒ¥é‡å­åŒ–ã¨æ¨è«–ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚è©³ç´°ãªã‚¬ã‚¤ãƒ‰ã¯[ã“ã¡ã‚‰](docs/en/quantization/kv_quant.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„
- \[2024/04\] TurboMindã®æœ€æ–°ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã«ã‚ˆã‚ŠGQAãŒå¼·åŒ–ã•ã‚Œã€[internlm2-20b](https://huggingface.co/internlm/internlm2-20b)ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ãŒ16+ RPSã«é”ã—ã€vLLMã®ç´„1.8å€ã®é€Ÿã•ã«ãªã‚Šã¾ã—ãŸ
- \[2024/04\] Qwen1.5-MOEãŠã‚ˆã³dbrxã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2024/03\] DeepSeek-VLã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2024/03\] VLMã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2024/02\] Qwen 1.5ã€Gemmaã€Mistralã€Mixtralã€Deepseek-MOEãªã©ã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2024/01\] [OpenAOE](https://github.com/InternLM/OpenAOE)ãŒ[LMDeployã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹](./docs/en/llm/api_server.md)ã¨ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«çµ±åˆã•ã‚Œã¾ã—ãŸ
- \[2024/01\] è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã€è¤‡æ•°ãƒã‚·ãƒ³ã€è¤‡æ•°ã‚«ãƒ¼ãƒ‰ã®æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚µãƒãƒ¼ãƒˆã€‚ä½¿ç”¨æ–¹æ³•ã¯[ã“ã¡ã‚‰](./docs/en/llm/proxy_server.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„
- \[2024/01\] [PyTorchæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³](./docs/en/inference/pytorch.md)ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€å®Œå…¨ã«Pythonã§é–‹ç™ºã•ã‚Œã¦ãŠã‚Šã€é–‹ç™ºè€…ã®éšœå£ã‚’ä¸‹ã’ã€æ–°æ©Ÿèƒ½ã‚„æŠ€è¡“ã®è¿…é€Ÿãªå®Ÿé¨“ã‚’å¯èƒ½ã«ã—ã¾ã™

</details>

<details close>
<summary><b>2023</b></summary>

- \[2023/12\] Turbomindã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2023/11\] Turbomindã¯hfãƒ¢ãƒ‡ãƒ«ã®ç›´æ¥èª­ã¿è¾¼ã¿ã‚’ã‚µãƒãƒ¼ãƒˆã€‚è©³ç´°ã¯[ã“ã¡ã‚‰](docs/en/inference/load_hf.md)ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„
- \[2023/11\] TurboMindã®ä¸»è¦ãªã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã€åŒ…æ‹¬çš„ãªPaged Attentionã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åˆ¶é™ã®ãªã„é«˜é€Ÿãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚«ãƒ¼ãƒãƒ«ã€2å€é€Ÿã„KV8ã‚«ãƒ¼ãƒãƒ«ã€Split-Kãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆFlash Decodingï¼‰ã€ãŠã‚ˆã³sm_75ã®W4A16æ¨è«–
- \[2023/09\] TurboMindã¯Qwen-14Bã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2023/09\] TurboMindã¯InternLM-20Bã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2023/09\] TurboMindã¯Code Llamaã®ã™ã¹ã¦ã®æ©Ÿèƒ½ã‚’ã‚µãƒãƒ¼ãƒˆï¼šã‚³ãƒ¼ãƒ‰è£œå®Œã€ã‚¤ãƒ³ãƒ•ã‚£ãƒªãƒ³ã‚°ã€ãƒãƒ£ãƒƒãƒˆ/ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã€Pythonã‚¹ãƒšã‚·ãƒ£ãƒªã‚¹ãƒˆã€‚ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¬ã‚¤ãƒ‰ã¯[ã“ã¡ã‚‰](./docs/en/llm/codellama.md)ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„
- \[2023/09\] TurboMindã¯Baichuan2-7Bã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2023/08\] TurboMindã¯flash-attention2ã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2023/08\] TurboMindã¯Qwen-7Bã€å‹•çš„NTK-RoPEã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€å‹•çš„logNã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2023/08\] TurboMindã¯Windowsã‚’ã‚µãƒãƒ¼ãƒˆï¼ˆtp=1ï¼‰
- \[2023/08\] TurboMindã¯4ãƒ“ãƒƒãƒˆæ¨è«–ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€FP16ã®2.4å€ã®é€Ÿã•ã§ã€æœ€é€Ÿã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹å®Ÿè£…ã§ã™ã€‚è©³ç´°ãªæƒ…å ±ã¯[ã“ã¡ã‚‰](docs/en/quantization/w4a16.md)ã®ã‚¬ã‚¤ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- \[2023/08\] LMDeployã¯[HuggingFace Hub](https://huggingface.co/lmdeploy)ã§æä¾›ã•ã‚Œã€ã™ãã«ä½¿ç”¨ã§ãã‚‹4ãƒ“ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã¾ã™
- \[2023/08\] LMDeployã¯[AWQ](https://arxiv.org/abs/2306.00978)ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ãŸ4ãƒ“ãƒƒãƒˆé‡å­åŒ–ã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2023/07\] TurboMindã¯GQAã‚’ä½¿ç”¨ã—ãŸLlama-2 70Bã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2023/07\] TurboMindã¯Llama-2 7B/13Bã‚’ã‚µãƒãƒ¼ãƒˆ
- \[2023/07\] TurboMindã¯InternLMã®ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—æ¨è«–ã‚’ã‚µãƒãƒ¼ãƒˆ

</details>

______________________________________________________________________

# ç´¹ä»‹

LMDeployã¯ã€[MMRazor](https://github.com/open-mmlab/mmrazor)ãŠã‚ˆã³[MMDeploy](https://github.com/open-mmlab/mmdeploy)ãƒãƒ¼ãƒ ã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸã€LLMã®åœ§ç¸®ã€ãƒ‡ãƒ—ãƒ­ã‚¤ã€ãŠã‚ˆã³ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã®ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ä¸»è¦ãªæ©Ÿèƒ½ã‚’å‚™ãˆã¦ã„ã¾ã™ï¼š

- **åŠ¹ç‡çš„ãªæ¨è«–**ï¼šLMDeployã¯ã€persistent batchï¼ˆé€£ç¶šãƒãƒƒãƒï¼‰ã€ãƒ–ãƒ­ãƒƒã‚¯åŒ–ã•ã‚ŒãŸKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€å‹•çš„åˆ†å‰²ã¨èåˆã€ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—ã€é«˜æ€§èƒ½ãªCUDAã‚«ãƒ¼ãƒãƒ«ãªã©ã®ä¸»è¦ãªæ©Ÿèƒ½ã‚’å°å…¥ã—ã€vLLMã‚ˆã‚Šã‚‚æœ€å¤§1.8å€ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æä¾›ã—ã¾ã™ã€‚

- **åŠ¹æœçš„ãªé‡å­åŒ–**ï¼šLMDeployã¯ã€é‡ã¿ã®ã¿ãŠã‚ˆã³k/vã®é‡å­åŒ–ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€4ãƒ“ãƒƒãƒˆã®æ¨è«–æ€§èƒ½ã¯FP16ã®2.4å€ã§ã™ã€‚é‡å­åŒ–ã®å“è³ªã¯OpenCompassã®è©•ä¾¡ã‚’é€šã˜ã¦ç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã€‚

- **ç°¡å˜ãªåˆ†æ•£ã‚µãƒ¼ãƒãƒ¼**ï¼šãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ†æ•£ã‚µãƒ¼ãƒ“ã‚¹ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€LMDeployã¯è¤‡æ•°ã®ãƒã‚·ãƒ³ãŠã‚ˆã³ã‚«ãƒ¼ãƒ‰ã«ã‚ãŸã‚‹ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚

- **å„ªã‚ŒãŸäº’æ›æ€§**ï¼šLMDeployã¯ã€[KV Cache Quant](docs/en/quantization/kv_quant.md)ã€[AWQ](docs/en/quantization/w4a16.md)ã€ãŠã‚ˆã³[Automatic Prefix Caching](docs/en/inference/turbomind_config.md)ã‚’åŒæ™‚ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

LMDeploy TurboMindã‚¨ãƒ³ã‚¸ãƒ³ã¯å“è¶Šã—ãŸæ¨è«–èƒ½åŠ›ã‚’æŒã¡ã€ã•ã¾ã–ã¾ãªè¦æ¨¡ã®ãƒ¢ãƒ‡ãƒ«ã§ã€vLLMã®1.36ã€œ1.85å€ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ¯ç§’å‡¦ç†ã—ã¾ã™ã€‚é™çš„æ¨è«–èƒ½åŠ›ã®é¢ã§ã¯ã€TurboMind 4ãƒ“ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®æ¨è«–é€Ÿåº¦ï¼ˆout token/sï¼‰ã¯FP16/BF16æ¨è«–ã‚’ã¯ã‚‹ã‹ã«ä¸Šå›ã‚Šã¾ã™ã€‚å°ã•ãªãƒãƒƒãƒã§ã¯ã€2.4å€ã«å‘ä¸Šã—ã¾ã™ã€‚

![v0 1 0-benchmark](https://github.com/InternLM/lmdeploy/assets/4560679/8e455cf1-a792-4fa8-91a2-75df96a2a5ba)

# ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«

<table>
<tbody>
<tr align="center" valign="middle">
<td>
  <b>LLMs</b>
</td>
<td>
  <b>VLMs</b>
</td>
<tr valign="top">
<td align="left" valign="top">
<ul>
  <li>Llama (7B - 65B)</li>
  <li>Llama2 (7B - 70B)</li>
  <li>Llama3 (8B, 70B)</li>
  <li>Llama3.1 (8B, 70B)</li>
  <li>Llama3.2 (1B, 3B)</li>
  <li>InternLM (7B - 20B)</li>
  <li>InternLM2 (7B - 20B)</li>
  <li>InternLM3 (8B)</li>
  <li>InternLM2.5 (7B)</li>
  <li>Qwen (1.8B - 72B)</li>
  <li>Qwen1.5 (0.5B - 110B)</li>
  <li>Qwen1.5 - MoE (0.5B - 72B)</li>
  <li>Qwen2 (0.5B - 72B)</li>
  <li>Qwen2-MoE (57BA14B)</li>
  <li>Qwen2.5 (0.5B - 32B)</li>
  <li>Qwen3, Qwen3-MoE</li>
  <li>Qwen3-Next(80B)</li>
  <li>Baichuan (7B)</li>
  <li>Baichuan2 (7B-13B)</li>
  <li>Code Llama (7B - 34B)</li>
  <li>ChatGLM2 (6B)</li>
  <li>GLM-4 (9B)</li>
  <li>GLM-4-0414 (9B, 32B)</li>
  <li>CodeGeeX4 (9B)</li>
  <li>YI (6B-34B)</li>
  <li>Mistral (7B)</li>
  <li>DeepSeek-MoE (16B)</li>
  <li>DeepSeek-V2 (16B, 236B)</li>
  <li>DeepSeek-V2.5 (236B)</li>
  <li>Mixtral (8x7B, 8x22B)</li>
  <li>Gemma (2B - 7B)</li>
  <li>StarCoder2 (3B - 15B)</li>
  <li>Phi-3-mini (3.8B)</li>
  <li>Phi-3.5-mini (3.8B)</li>
  <li>Phi-3.5-MoE (16x3.8B)</li>
  <li>Phi-4-mini (3.8B)</li>
  <li>MiniCPM3 (4B)</li>
  <li>SDAR (1.7B-30B)</li>
</ul>
</td>
<td>
<ul>
  <li>LLaVA(1.5,1.6) (7B-34B)</li>
  <li>InternLM-XComposer2 (7B, 4khd-7B)</li>
  <li>InternLM-XComposer2.5 (7B)</li>
  <li>Qwen-VL (7B)</li>
  <li>Qwen2-VL (2B, 7B, 72B)</li>
  <li>Qwen2.5-VL (3B, 7B, 72B)</li>
  <li>Qwen3-VL (2B - 235B)</li>
  <li>DeepSeek-VL (7B)</li>
  <li>DeepSeek-VL2 (3B, 16B, 27B)</li>
  <li>InternVL-Chat (v1.1-v1.5)</li>
  <li>InternVL2 (1B-76B)</li>
  <li>InternVL2.5(MPO) (1B-78B)</li>
  <li>InternVL3 (1B-78B)</li>
  <li>InternVL3.5 (1B-241BA28B)</li>
  <li>Intern-S1 (241B)</li>
  <li>Intern-S1-mini (8.3B)</li>
  <li>Mono-InternVL (2B)</li>
  <li>ChemVLM (8B-26B)</li>
  <li>CogVLM-Chat (17B)</li>
  <li>CogVLM2-Chat (19B)</li>
  <li>MiniCPM-Llama3-V-2_5</li>
  <li>MiniCPM-V-2_6</li>
  <li>Phi-3-vision (4.2B)</li>
  <li>Phi-3.5-vision (4.2B)</li>
  <li>GLM-4V (9B)</li>
  <li>GLM-4.1V-Thinking (9B)</li>
  <li>Llama3.2-vision (11B, 90B)</li>
  <li>Molmo (7B-D,72B)</li>
  <li>Gemma3 (1B - 27B)</li>
  <li>Llama4 (Scout, Maverick)</li>
</ul>
</td>
</tr>
</tbody>
</table>

LMDeployã¯ã€[TurboMind](./docs/en/inference/turbomind.md)ãŠã‚ˆã³[PyTorch](./docs/en/inference/pytorch.md)ã®2ã¤ã®æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚ãã‚Œãã‚Œç•°ãªã‚‹ç„¦ç‚¹ã‚’æŒã£ã¦ã„ã¾ã™ã€‚å‰è€…ã¯æ¨è«–æ€§èƒ½ã®ç©¶æ¥µã®æœ€é©åŒ–ã‚’ç›®æŒ‡ã—ã€å¾Œè€…ã¯å®Œå…¨ã«Pythonã§é–‹ç™ºã•ã‚Œã¦ãŠã‚Šã€é–‹ç™ºè€…ã®éšœå£ã‚’ä¸‹ã’ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚

ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ç¨®é¡ã‚„æ¨è«–ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«é•ã„ãŒã‚ã‚Šã¾ã™ã€‚å„ã‚¨ãƒ³ã‚¸ãƒ³ã®èƒ½åŠ›ã«ã¤ã„ã¦ã¯[ã“ã®è¡¨](./docs/en/supported_models/supported_models.md)ã‚’å‚ç…§ã—ã€å®Ÿéš›ã®ãƒ‹ãƒ¼ã‚ºã«æœ€é©ãªã‚‚ã®ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

# ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Dh-YlSwg78ZO3AlleO441NF_QP2shs95#scrollTo=YALmXnwCG1pQ)

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ã‚¯ãƒªãƒ¼ãƒ³ãªcondaç’°å¢ƒï¼ˆPython 3.9 - 3.12ï¼‰ã§lmdeployã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

```shell
conda create -n lmdeploy python=3.10 -y
conda activate lmdeploy
pip install lmdeploy
```

v0.3.0ã‹ã‚‰ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®äº‹å‰æ§‹ç¯‰æ¸ˆã¿ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯CUDA 12ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã¦ã„ã¾ã™ã€‚
CUDA 11+ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«é–¢ã™ã‚‹æƒ…å ±ã€ã¾ãŸã¯ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ“ãƒ«ãƒ‰æ‰‹é †ã«ã¤ã„ã¦ã¯ã€[ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¬ã‚¤ãƒ‰ã‚’](docs/en/get_started/installation.md)å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒæ¨è«–

```python
import lmdeploy
with lmdeploy.pipeline("internlm/internlm3-8b-instruct") as pipe:
    response = pipe(["Hi, pls intro yourself", "Shanghai is"])
    print(response)
```

> \[!NOTE\]
> ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€LMDeployã¯HuggingFaceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ModelScopeã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€`pip install modelscope`ã‚³ãƒãƒ³ãƒ‰ã§ModelScopeã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š
>
> `export LMDEPLOY_USE_MODELSCOPE=True`
>
> openMind Hubã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€`pip install openmind_hub`ã‚³ãƒãƒ³ãƒ‰ã§openMind Hubã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š
>
> `export LMDEPLOY_USE_OPENMIND_HUB=True`

æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±ã¯[ã“ã¡ã‚‰](./docs/en/llm/pipeline.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

# ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

LMDeployã®åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€[getting_started](docs/en/get_started/get_started.md)ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

è©³ç´°ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰ã¨é«˜åº¦ãªã‚¬ã‚¤ãƒ‰ã«ã¤ã„ã¦ã¯ã€[ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://lmdeploy.readthedocs.io/en/latest/)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼š

- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰
  - [LLMæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](./docs/en/llm/pipeline.md) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Dh-YlSwg78ZO3AlleO441NF_QP2shs95#scrollTo=YALmXnwCG1pQ)
  - [VLMæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](./docs/en/multi_modal/vl_pipeline.md) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1nKLfnPeDA3p-FMNw2NhI-KOpk7-nlNjF?usp=sharing)
  - [LLMã‚µãƒ¼ãƒ“ãƒ³ã‚°](docs/en/llm/api_server.md)
  - [VLMã‚µãƒ¼ãƒ“ãƒ³ã‚°](docs/en/multi_modal/api_server_vl.md)
  - [é‡å­åŒ–](docs/en/quantization)
- é«˜åº¦ãªã‚¬ã‚¤ãƒ‰
  - [æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ - TurboMind](docs/en/inference/turbomind.md)
  - [æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ - PyTorch](docs/en/inference/pytorch.md)
  - [ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ](docs/en/advance/chat_template.md)
  - [æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ ](docs/en/advance/pytorch_new_model.md)
  - gemmãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
  - [é•·æ–‡æ¨è«–](docs/en/advance/long_context.md)
  - [ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹](docs/en/llm/proxy_server.md)

# ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

- LMDeployã‚’ä½¿ç”¨ã—ã¦NVIDIA Jetsonãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§LLMã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãƒ‡ãƒ—ãƒ­ã‚¤ï¼š[LMDeploy-Jetson](https://github.com/BestAnHongjun/LMDeploy-Jetson)
- LMDeployã¨BentoMLã‚’ä½¿ç”¨ã—ã¦LLMã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ãŸã‚ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼š[BentoLMDeploy](https://github.com/bentoml/BentoLMDeploy)

# è²¢çŒ®

LMDeployã¸ã®ã™ã¹ã¦ã®è²¢çŒ®ã«æ„Ÿè¬ã—ã¾ã™ã€‚è²¢çŒ®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«ã¤ã„ã¦ã¯ã€[CONTRIBUTING.md](.github/CONTRIBUTING.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

# è¬è¾

- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [llm-awq](https://github.com/mit-han-lab/llm-awq)
- [vLLM](https://github.com/vllm-project/vllm)
- [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII)

# å¼•ç”¨

```bibtex
@misc{2023lmdeploy,
    title={LMDeploy: A Toolkit for Compressing, Deploying, and Serving LLM},
    author={LMDeploy Contributors},
    howpublished = {\url{https://github.com/InternLM/lmdeploy}},
    year={2023}
}
```

# ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯[Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹](LICENSE)ã®ä¸‹ã§ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚
