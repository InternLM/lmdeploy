<div align="center">
  <img src="docs/en/_static/image/lmdeploy-logo.svg" width="450"/>

[![PyPI](https://img.shields.io/pypi/v/lmdeploy)](https://pypi.org/project/lmdeploy)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lmdeploy)
[![license](https://img.shields.io/github/license/InternLM/lmdeploy.svg)](https://github.com/InternLM/lmdeploy/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)
[![open issues](https://img.shields.io/github/issues-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)

[ğŸ“˜Documentation](https://lmdeploy.readthedocs.io/zh-cn/latest/) |
[ğŸ› ï¸Quick Start](https://lmdeploy.readthedocs.io/zh-cn/latest/get_started/get_started.html) |
[ğŸ¤”Reporting Issues](https://github.com/InternLM/lmdeploy/issues/new/choose)

[English](README.md) | ç®€ä½“ä¸­æ–‡ | [æ—¥æœ¬èª](README_ja.md)

ğŸ‘‹ join us on [![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=wechat&label=WeChat)](https://cdn.vansin.top/internlm/lmdeploy.jpg)
[![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=twitter&label=Twitter)](https://twitter.com/intern_lm)
[![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=discord&label=Discord)](https://discord.gg/xa29JuW87d)

</div>

______________________________________________________________________

## æœ€æ–°è¿›å±• ğŸ‰

<details open>
<summary><b>2025</b></summary>
</details>

- ã€2025å¹´9æœˆã€‘TurboMind å¼•æ“æ”¯æŒ MXFP4ï¼Œé€‚ç”¨äº NVIDIA V100 åŠä»¥ä¸Š GPUã€‚åœ¨ H800 ä¸Šæ¨ç† openai gpt-oss æ¨¡å‹ï¼Œæ€§èƒ½å¯è¾¾ vLLM çš„ 1.5å€ï¼
- ã€2025å¹´6æœˆã€‘æ·±åº¦ä¼˜åŒ– FP8 MoE æ¨¡å‹æ¨ç†
- ã€2025å¹´6æœˆã€‘é›†æˆ[DLSlime](https://github.com/DeepLink-org/DLSlime)å’Œ[Mooncake](https://github.com/kvcache-ai/Mooncake)ï¼Œå®ç°DeepSeek PDåˆ†ç¦»éƒ¨ç½²ï¼Œå‘ä¸¤ä¸ªå›¢é˜Ÿè¡¨ç¤ºè¯šæŒšçš„æ„Ÿè°¢ï¼
- ã€2025å¹´4æœˆã€‘é›†æˆdeepseek-aiç»„ä»¶FlashMLAã€DeepGemmã€DeepEPã€MicroBatchã€eplbç­‰ï¼Œæå‡DeepSeekæ¨ç†æ€§èƒ½
- ã€2025å¹´1æœˆã€‘æ–°å¢å¯¹DeepSeek V3åŠR1çš„æ”¯æŒ

<details close>
<summary><b>2024</b></summary>

- \[2024/11\] PyTorch engine æ”¯æŒ Mono-InternVL æ¨¡å‹
- \[2024/10\] PyTorchEngine åœ¨ ascend å¹³å°ä¸Šæ”¯æŒäº†å›¾æ¨¡å¼ï¼Œæ¨ç†æ€§èƒ½æé«˜äº† 1 å€
- \[2024/09\] LMDeploy PyTorchEngine å¢åŠ äº†å¯¹ [åä¸º Ascend](docs/zh_cn/get_started/ascend/get_started.md) çš„æ”¯æŒã€‚æ”¯æŒçš„æ¨¡å‹è¯·è§[è¿™é‡Œ](docs/zh_cn/supported_models/supported_models.md)
- \[2024/09\] é€šè¿‡å¼•å…¥ CUDA Graphï¼ŒLMDeploy PyTorchEngine åœ¨ Llama3-8B æ¨ç†ä¸Šå®ç°äº† 1.3 å€çš„åŠ é€Ÿ
- \[2024/08\] LMDeployç°å·²é›†æˆè‡³ [modelscope/swift](https://github.com/modelscope/swift)ï¼Œæˆä¸º VLMs æ¨ç†çš„é»˜è®¤åŠ é€Ÿå¼•æ“
- \[2024/07\] æ”¯æŒ Llama3.1 8B å’Œ 70B æ¨¡å‹ï¼Œä»¥åŠå·¥å…·è°ƒç”¨åŠŸèƒ½
- \[2024/07\] æ”¯æŒ [InternVL2](docs/zh_cn/multi_modal/internvl.md) å…¨ç³»åˆ—æ¨¡å‹ï¼Œ[InternLM-XComposer2.5](docs/zh_cn/multi_modal/xcomposer2d5.md) æ¨¡å‹å’Œ InternLM2.5 çš„ [function call åŠŸèƒ½](docs/zh_cn/llm/api_server_tools.md)
- \[2024/06\] PyTorch engine æ”¯æŒäº† DeepSeek-V2 å’Œè‹¥å¹² VLM æ¨¡å‹æ¨ç†, æ¯”å¦‚ CogVLM2ï¼ŒMini-InternVLï¼ŒLlaVA-Next
- \[2024/05\] åœ¨å¤š GPU ä¸Šéƒ¨ç½² VLM æ¨¡å‹æ—¶ï¼Œæ”¯æŒæŠŠè§†è§‰éƒ¨åˆ†çš„æ¨¡å‹å‡åˆ†åˆ°å¤šå¡ä¸Š
- \[2024/05\] æ”¯æŒInternVL v1.5, LLaVa, InternLMXComposer2 ç­‰ VLMs æ¨¡å‹çš„ 4bit æƒé‡é‡åŒ–å’Œæ¨ç†
- \[2024/04\] æ”¯æŒ Llama3 å’Œ InternVL v1.1, v1.2ï¼ŒMiniGeminiï¼ŒInternLM-XComposer2 ç­‰ VLM æ¨¡å‹
- \[2024/04\] TurboMind æ”¯æŒ kv cache int4/int8 åœ¨çº¿é‡åŒ–å’Œæ¨ç†ï¼Œé€‚ç”¨å·²æ”¯æŒçš„æ‰€æœ‰å‹å·æ˜¾å¡ã€‚è¯¦æƒ…è¯·å‚è€ƒ[è¿™é‡Œ](docs/zh_cn/quantization/kv_quant.md)
- \[2024/04\] TurboMind å¼•æ“å‡çº§ï¼Œä¼˜åŒ– GQA æ¨ç†ã€‚[internlm2-20b](https://huggingface.co/internlm/internlm2-20b) æ¨ç†é€Ÿåº¦è¾¾ 16+ RPSï¼Œçº¦æ˜¯ vLLM çš„ 1.8 å€
- \[2024/04\] æ”¯æŒ Qwen1.5-MOE å’Œ dbrx.
- \[2024/03\] æ”¯æŒ DeepSeek-VL çš„ç¦»çº¿æ¨ç† pipeline å’Œæ¨ç†æœåŠ¡
- \[2024/03\] æ”¯æŒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç¦»çº¿æ¨ç† pipeline å’Œæ¨ç†æœåŠ¡
- \[2024/02\] æ”¯æŒ Qwen 1.5ã€Gemmaã€Mistralã€Mixtralã€Deepseek-MOE ç­‰æ¨¡å‹
- \[2024/01\] [OpenAOE](https://github.com/InternLM/OpenAOE) å‘å¸ƒï¼Œæ”¯æŒæ— ç¼æ¥å…¥[LMDeploy Serving Service](docs/zh_cn/llm/api_server.md)
- \[2024/01\] æ”¯æŒå¤šæ¨¡å‹ã€å¤šæœºã€å¤šå¡æ¨ç†æœåŠ¡ã€‚ä½¿ç”¨æ–¹æ³•è¯·å‚è€ƒ[æ­¤å¤„](docs/zh_cn/llm/proxy_server.md)
- \[2024/01\] å¢åŠ  [PyTorch æ¨ç†å¼•æ“](./docs/zh_cn/inference/pytorch.md)ï¼Œä½œä¸º TurboMind å¼•æ“çš„è¡¥å……ã€‚å¸®åŠ©é™ä½å¼€å‘é—¨æ§›ï¼Œå’Œå¿«é€Ÿå®éªŒæ–°ç‰¹æ€§ã€æ–°æŠ€æœ¯

</details>

<details close>
<summary><b>2023</b></summary>

- \[2023/12\] Turbomind æ”¯æŒå¤šæ¨¡æ€è¾“å…¥
- \[2023/11\] Turbomind æ”¯æŒç›´æ¥è¯»å– Huggingface æ¨¡å‹ã€‚ç‚¹å‡»[è¿™é‡Œ](docs/zh_cn/inference/load_hf.md)æŸ¥çœ‹ä½¿ç”¨æ–¹æ³•
- \[2023/11\] TurboMind é‡ç£…å‡çº§ã€‚åŒ…æ‹¬ï¼šPaged Attentionã€æ›´å¿«çš„ä¸”ä¸å—åºåˆ—æœ€å¤§é•¿åº¦é™åˆ¶çš„ attention kernelã€2+å€å¿«çš„ KV8 kernelsã€Split-K decoding (Flash Decoding) å’Œ æ”¯æŒ sm_75 æ¶æ„çš„ W4A16
- \[2023/09\] TurboMind æ”¯æŒ Qwen-14B
- \[2023/09\] TurboMind æ”¯æŒ InternLM-20B æ¨¡å‹
- \[2023/09\] TurboMind æ”¯æŒ Code Llama æ‰€æœ‰åŠŸèƒ½ï¼šä»£ç ç»­å†™ã€å¡«ç©ºã€å¯¹è¯ã€Pythonä¸“é¡¹ã€‚ç‚¹å‡»[è¿™é‡Œ](./docs/zh_cn/llm/codellama.md)é˜…è¯»éƒ¨ç½²æ–¹æ³•
- \[2023/09\] TurboMind æ”¯æŒ Baichuan2-7B
- \[2023/08\] TurboMind æ”¯æŒ flash-attention2
- \[2023/08\] TurboMind æ”¯æŒ Qwen-7Bï¼ŒåŠ¨æ€NTK-RoPEç¼©æ”¾ï¼ŒåŠ¨æ€logNç¼©æ”¾
- \[2023/08\] TurboMind æ”¯æŒ Windows (tp=1)
- \[2023/08\] TurboMind æ”¯æŒ 4-bit æ¨ç†ï¼Œé€Ÿåº¦æ˜¯ FP16 çš„ 2.4 å€ï¼Œæ˜¯ç›®å‰æœ€å¿«çš„å¼€æºå®ç°ã€‚éƒ¨ç½²æ–¹å¼è¯·çœ‹[è¿™é‡Œ](docs/zh_cn/quantization/w4a16.md)
- \[2023/08\] LMDeploy å¼€é€šäº† [HuggingFace Hub](https://huggingface.co/lmdeploy) ï¼Œæä¾›å¼€ç®±å³ç”¨çš„ 4-bit æ¨¡å‹
- \[2023/08\] LMDeploy æ”¯æŒä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•è¿›è¡Œ 4-bit é‡åŒ–
- \[2023/07\] TurboMind æ”¯æŒä½¿ç”¨ GQA çš„ Llama-2 70B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ Llama-2 7B/13B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ InternLM çš„ Tensor Parallel æ¨ç†

</details>
______________________________________________________________________

# ç®€ä»‹

LMDeploy ç”± [MMDeploy](https://github.com/open-mmlab/mmdeploy) å’Œ [MMRazor](https://github.com/open-mmlab/mmrazor) å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚
è¿™ä¸ªå¼ºå¤§çš„å·¥å…·ç®±æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

- **é«˜æ•ˆçš„æ¨ç†**ï¼šLMDeploy å¼€å‘äº† Persistent Batch(å³ Continuous Batch)ï¼ŒBlocked K/V Cacheï¼ŒåŠ¨æ€æ‹†åˆ†å’Œèåˆï¼Œå¼ é‡å¹¶è¡Œï¼Œé«˜æ•ˆçš„è®¡ç®— kernelç­‰é‡è¦ç‰¹æ€§ã€‚æ¨ç†æ€§èƒ½æ˜¯ vLLM çš„ 1.8 å€

- **å¯é çš„é‡åŒ–**ï¼šLMDeploy æ”¯æŒæƒé‡é‡åŒ–å’Œ k/v é‡åŒ–ã€‚4bit æ¨¡å‹æ¨ç†æ•ˆç‡æ˜¯ FP16 ä¸‹çš„ 2.4 å€ã€‚é‡åŒ–æ¨¡å‹çš„å¯é æ€§å·²é€šè¿‡ OpenCompass è¯„æµ‹å¾—åˆ°å……åˆ†éªŒè¯ã€‚

- **ä¾¿æ·çš„æœåŠ¡**ï¼šé€šè¿‡è¯·æ±‚åˆ†å‘æœåŠ¡ï¼ŒLMDeploy æ”¯æŒå¤šæ¨¡å‹åœ¨å¤šæœºã€å¤šå¡ä¸Šçš„æ¨ç†æœåŠ¡ã€‚

- **å“è¶Šçš„å…¼å®¹æ€§**: LMDeploy æ”¯æŒ [KV Cache é‡åŒ–](docs/zh_cn/quantization/kv_quant.md), [AWQ](docs/zh_cn/quantization/w4a16.md) å’Œ [Automatic Prefix Caching](docs/zh_cn/inference/turbomind_config.md) åŒæ—¶ä½¿ç”¨ã€‚

# æ€§èƒ½

LMDeploy TurboMind å¼•æ“æ‹¥æœ‰å“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨å„ç§è§„æ¨¡çš„æ¨¡å‹ä¸Šï¼Œæ¯ç§’å¤„ç†çš„è¯·æ±‚æ•°æ˜¯ vLLM çš„ 1.36 ~ 1.85 å€ã€‚åœ¨é™æ€æ¨ç†èƒ½åŠ›æ–¹é¢ï¼ŒTurboMind 4bit æ¨¡å‹æ¨ç†é€Ÿåº¦ï¼ˆout token/sï¼‰è¿œé«˜äº FP16/BF16 æ¨ç†ã€‚åœ¨å° batch æ—¶ï¼Œæé«˜åˆ° 2.4 å€ã€‚

![v0 1 0-benchmark](https://github.com/InternLM/lmdeploy/assets/4560679/8e455cf1-a792-4fa8-91a2-75df96a2a5ba)

# æ”¯æŒçš„æ¨¡å‹

<table>
<tbody>
<tr align="center" valign="middle">
<td>
  <b>LLMs</b>
</td>
<td>
  <b>VLMs</b>
</td>
<tr valign="top">
<td align="left" valign="top">
<ul>
  <li>Llama (7B - 65B)</li>
  <li>Llama2 (7B - 70B)</li>
  <li>Llama3 (8B, 70B)</li>
  <li>Llama3.1 (8B, 70B)</li>
  <li>Llama3.2 (1B, 3B)</li>
  <li>InternLM (7B - 20B)</li>
  <li>InternLM2 (7B - 20B)</li>
  <li>InternLM3 (8B)</li>
  <li>InternLM2.5 (7B)</li>
  <li>Qwen (1.8B - 72B)</li>
  <li>Qwen1.5 (0.5B - 110B)</li>
  <li>Qwen1.5 - MoE (0.5B - 72B)</li>
  <li>Qwen2 (0.5B - 72B)</li>
  <li>Qwen2-MoE (57BA14B)</li>
  <li>Qwen2.5 (0.5B - 32B)</li>
  <li>Qwen3, Qwen3-MoE</li>
  <li>Qwen3-Next(80B)</li>
  <li>Baichuan (7B)</li>
  <li>Baichuan2 (7B-13B)</li>
  <li>Code Llama (7B - 34B)</li>
  <li>ChatGLM2 (6B)</li>
  <li>GLM-4 (9B)</li>
  <li>GLM-4-0414 (9B, 32B)</li>
  <li>CodeGeeX4 (9B)</li>
  <li>YI (6B-34B)</li>
  <li>Mistral (7B)</li>
  <li>DeepSeek-MoE (16B)</li>
  <li>DeepSeek-V2 (16B, 236B)</li>
  <li>DeepSeek-V2.5 (236B)</li>
  <li>Mixtral (8x7B, 8x22B)</li>
  <li>Gemma (2B - 7B)</li>
  <li>StarCoder2 (3B - 15B)</li>
  <li>Phi-3-mini (3.8B)</li>
  <li>Phi-3.5-mini (3.8B)</li>
  <li>Phi-3.5-MoE (16x3.8B)</li>
  <li>Phi-4-mini (3.8B)</li>
  <li>MiniCPM3 (4B)</li>
  <li>SDAR (1.7B-30B)</li>
  <li>gpt-oss (20B, 120B)</li>
</ul>
</td>
<td>
<ul>
  <li>LLaVA(1.5,1.6) (7B-34B)</li>
  <li>InternLM-XComposer2 (7B, 4khd-7B)</li>
  <li>InternLM-XComposer2.5 (7B)</li>
  <li>Qwen-VL (7B)</li>
  <li>Qwen2-VL (2B, 7B, 72B)</li>
  <li>Qwen2.5-VL (3B, 7B, 72B)</li>
  <li>Qwen3-VL (2B - 235B)</li>
  <li>DeepSeek-VL (7B)</li>
  <li>DeepSeek-VL2 (3B, 16B, 27B)</li>
  <li>InternVL-Chat (v1.1-v1.5)</li>
  <li>InternVL2 (1B-76B)</li>
  <li>InternVL2.5(MPO) (1B-78B)</li>
  <li>InternVL3 (1B-78B)</li>
  <li>InternVL3.5 (1B-241BA28B)</li>
  <li>Intern-S1 (241B)</li>
  <li>Intern-S1-mini (8.3B)</li>
  <li>Mono-InternVL (2B)</li>
  <li>ChemVLM (8B-26B)</li>
  <li>CogVLM-Chat (17B)</li>
  <li>CogVLM2-Chat (19B)</li>
  <li>MiniCPM-Llama3-V-2_5</li>
  <li>MiniCPM-V-2_6</li>
  <li>Phi-3-vision (4.2B)</li>
  <li>Phi-3.5-vision (4.2B)</li>
  <li>GLM-4V (9B)</li>
  <li>GLM-4.1V-Thinking (9B)</li>
  <li>Llama3.2-vision (11B, 90B)</li>
  <li>Molmo (7B-D,72B)</li>
  <li>Gemma3 (1B - 27B)</li>
  <li>Llama4 (Scout, Maverick)</li>
</ul>
</td>
</tr>
</tbody>
</table>

LMDeploy æ”¯æŒ 2 ç§æ¨ç†å¼•æ“ï¼š [TurboMind](./docs/zh_cn/inference/turbomind.md) å’Œ [PyTorch](./docs/zh_cn/inference/pytorch.md)ï¼Œå®ƒä»¬ä¾§é‡ä¸åŒã€‚å‰è€…è¿½æ±‚æ¨ç†æ€§èƒ½çš„æè‡´ä¼˜åŒ–ï¼Œåè€…çº¯ç”¨pythonå¼€å‘ï¼Œç€é‡é™ä½å¼€å‘è€…çš„é—¨æ§›ã€‚

å®ƒä»¬åœ¨æ”¯æŒçš„æ¨¡å‹ç±»åˆ«ã€è®¡ç®—ç²¾åº¦æ–¹é¢æœ‰æ‰€å·®åˆ«ã€‚ç”¨æˆ·å¯å‚è€ƒ[è¿™é‡Œ](./docs/zh_cn/supported_models/supported_models.md), æŸ¥é˜…æ¯ä¸ªæ¨ç†å¼•æ“çš„èƒ½åŠ›ï¼Œå¹¶æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„ã€‚

# å¿«é€Ÿå¼€å§‹ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Dh-YlSwg78ZO3AlleO441NF_QP2shs95#scrollTo=YALmXnwCG1pQ)

## å®‰è£…

æˆ‘ä»¬æ¨èåœ¨ä¸€ä¸ªå¹²å‡€çš„condaç¯å¢ƒä¸‹ï¼ˆpython3.9 - 3.12ï¼‰ï¼Œå®‰è£… lmdeployï¼š

```shell
conda create -n lmdeploy python=3.10 -y
conda activate lmdeploy
pip install lmdeploy
```

è‡ª v0.3.0 ç‰ˆæœ¬èµ·ï¼Œé»˜è®¤é¢„ç¼–è¯‘åŒ…åŸºäº **CUDA 12** ç¼–è¯‘ã€‚

è‹¥ä½¿ç”¨ GeForce RTX 50 ç³»åˆ—æ˜¾å¡ï¼Œè¯·å®‰è£…åŸºäº **CUDA 12.8** ç¼–è¯‘çš„ LMDeploy é¢„ç¼–è¯‘åŒ…ã€‚

```shell
export LMDEPLOY_VERSION=0.10.2
export PYTHON_VERSION=310
pip install https://github.com/InternLM/lmdeploy/releases/download/v${LMDEPLOY_VERSION}/lmdeploy-${LMDEPLOY_VERSION}+cu128-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux2014_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu128
```

å¦‚æœéœ€è¦åœ¨ CUDA 11+ ä¸‹å®‰è£… LMDeployï¼Œæˆ–è€…æºç å®‰è£… LMDeployï¼Œè¯·å‚è€ƒ[å®‰è£…æ–‡æ¡£](docs/zh_cn/get_started/installation.md)

## ç¦»çº¿æ‰¹å¤„ç†

```python
import lmdeploy
with lmdeploy.pipeline("internlm/internlm3-8b-instruct") as pipe:
    response = pipe(["Hi, pls intro yourself", "Shanghai is"])
    print(response)
```

> \[!NOTE\]
> LMDeploy é»˜è®¤ä» HuggingFace ä¸Šé¢ä¸‹è½½æ¨¡å‹ï¼Œå¦‚æœè¦ä» ModelScope ä¸Šé¢ä¸‹è½½æ¨¡å‹ï¼Œè¯·é€šè¿‡å‘½ä»¤ `pip install modelscope` å®‰è£…ModelScopeï¼Œå¹¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
>
> `export LMDEPLOY_USE_MODELSCOPE=True`
>
> å¦‚æœè¦ä» openMind Hub ä¸Šé¢ä¸‹è½½æ¨¡å‹ï¼Œè¯·é€šè¿‡å‘½ä»¤ `pip install openmind_hub` å®‰è£…openMind Hubï¼Œå¹¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
>
> `export LMDEPLOY_USE_OPENMIND_HUB=True`

å…³äº pipeline çš„æ›´å¤šæ¨ç†å‚æ•°è¯´æ˜ï¼Œè¯·å‚è€ƒ[è¿™é‡Œ](docs/zh_cn/llm/pipeline.md)

# ç”¨æˆ·æ•™ç¨‹

è¯·é˜…è¯»[å¿«é€Ÿä¸Šæ‰‹](docs/zh_cn/get_started/get_started.md)ç« èŠ‚ï¼Œäº†è§£ LMDeploy çš„åŸºæœ¬ç”¨æ³•ã€‚

ä¸ºäº†å¸®åŠ©ç”¨æˆ·æ›´è¿›ä¸€æ­¥äº†è§£ LMDeployï¼Œæˆ‘ä»¬å‡†å¤‡äº†ç”¨æˆ·æŒ‡å—å’Œè¿›é˜¶æŒ‡å—ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„[æ–‡æ¡£](https://lmdeploy.readthedocs.io/zh-cn/latest/)ï¼š

- ç”¨æˆ·æŒ‡å—
  - [LLM æ¨ç† pipeline](docs/zh_cn/llm/pipeline.md) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Dh-YlSwg78ZO3AlleO441NF_QP2shs95#scrollTo=YALmXnwCG1pQ)
  - [VLM æ¨ç† pipeline](docs/zh_cn/multi_modal/vl_pipeline.md) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1nKLfnPeDA3p-FMNw2NhI-KOpk7-nlNjF?usp=sharing)
  - [LLM æ¨ç†æœåŠ¡](docs/zh_cn/llm/api_server.md)
  - [VLM æ¨ç†æœåŠ¡](docs/zh_cn/multi_modal/api_server_vl.md)
  - [æ¨¡å‹é‡åŒ–](./docs/zh_cn/quantization)
- è¿›é˜¶æŒ‡å—
  - [æ¨ç†å¼•æ“ - TurboMind](./docs/zh_cn/inference/turbomind.md)
  - [æ¨ç†å¼•æ“ - PyTorch](./docs/zh_cn/inference/pytorch.md)
  - [è‡ªå®šä¹‰å¯¹è¯æ¨¡æ¿](./docs/zh_cn/advance/chat_template.md)
  - [æ”¯æŒæ–°æ¨¡å‹](./docs/zh_cn/advance/pytorch_new_model.md)
  - gemm tuning
  - [é•¿æ–‡æœ¬æ¨ç†](./docs/zh_cn/advance/long_context.md)
  - [å¤šæ¨¡å‹æ¨ç†æœåŠ¡](docs/zh_cn/llm/proxy_server.md)

# ç¤¾åŒºé¡¹ç›®

- ä½¿ç”¨LMDeployåœ¨è‹±ä¼Ÿè¾¾Jetsonç³»åˆ—æ¿å¡éƒ¨ç½²å¤§æ¨¡å‹ï¼š[LMDeploy-Jetson](https://github.com/BestAnHongjun/LMDeploy-Jetson)
- ä½¿ç”¨ LMDeploy å’Œ BentoML éƒ¨ç½²å¤§æ¨¡å‹çš„ç¤ºä¾‹é¡¹ç›®ï¼š[BentoLMDeploy](https://github.com/bentoml/BentoLMDeploy)

# è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ LMDeploy æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚è¯·å‚è€ƒ[è´¡çŒ®æŒ‡å—](.github/CONTRIBUTING.md)æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

# è‡´è°¢

- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [llm-awq](https://github.com/mit-han-lab/llm-awq)
- [vLLM](https://github.com/vllm-project/vllm)
- [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII)

# å¼•ç”¨

```bibtex
@misc{2023lmdeploy,
    title={LMDeploy: A Toolkit for Compressing, Deploying, and Serving LLM},
    author={LMDeploy Contributors},
    howpublished = {\url{https://github.com/InternLM/lmdeploy}},
    year={2023}
}
```

```bibtex
@article{zhang2025efficient,
  title={Efficient Mixed-Precision Large Language Model Inference with TurboMind},
  author={Zhang, Li and Jiang, Youhe and He, Guoliang and Chen, Xin and Lv, Han and Yao, Qian and Fu, Fangcheng and Chen, Kai},
  journal={arXiv preprint arXiv:2508.15601},
  year={2025}
}
```

# å¼€æºè®¸å¯è¯

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache 2.0 å¼€æºè®¸å¯è¯](LICENSE)ã€‚
