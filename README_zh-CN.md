<div align="center">
  <img src="docs/en/_static/image/lmdeploy-logo.svg" width="450"/>

[![docs](https://img.shields.io/badge/docs-latest-blue)](https://lmdeploy.readthedocs.io/zh-cn/latest/)
[![badge](https://github.com/InternLM/lmdeploy/workflows/lint/badge.svg)](https://github.com/InternLM/lmdeploy/actions)
[![PyPI](https://img.shields.io/pypi/v/lmdeploy)](https://pypi.org/project/lmdeploy)
[![license](https://img.shields.io/github/license/InternLM/lmdeploy.svg)](https://github.com/InternLM/lmdeploy/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)
[![open issues](https://img.shields.io/github/issues-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)

[English](README.md) | ç®€ä½“ä¸­æ–‡

</div>

<p align="center">
    ğŸ‘‹ join us on <a href="https://twitter.com/intern_lm" target="_blank">Twitter</a>, <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> and <a href="https://r.vansin.top/?r=internwx" target="_blank">WeChat</a>
</p>

______________________________________________________________________

## æœ€æ–°è¿›å±• ğŸ‰

<details open>
<summary><b>2024</b></summary>

- \[2024/01\] æ”¯æŒå¤šæ¨¡å‹ã€å¤šæœºã€å¤šå¡æ¨ç†æœåŠ¡ã€‚ä½¿ç”¨æ–¹æ³•è¯·å‚è€ƒ[æ­¤å¤„](./docs/zh_cn/serving/proxy_server.md)
- \[2024/01\] å¢åŠ  [PyTorch æ¨ç†å¼•æ“](./docs/zh_cn/inference/pytorch.md)ï¼Œä½œä¸º TurboMind å¼•æ“çš„è¡¥å……ã€‚å¸®åŠ©é™ä½å¼€å‘é—¨æ§›ï¼Œå’Œå¿«é€Ÿå®éªŒæ–°ç‰¹æ€§ã€æ–°æŠ€æœ¯

</details>

<details close>
<summary><b>2023</b></summary>

- \[2023/12\] Turbomind æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€‚[Gradio Demo](./examples/vl/README.md)
- \[2023/11\] Turbomind æ”¯æŒç›´æ¥è¯»å– Huggingface æ¨¡å‹ã€‚ç‚¹å‡»[è¿™é‡Œ](docs/zh_cn/inference/load_hf.md)æŸ¥çœ‹ä½¿ç”¨æ–¹æ³•
- \[2023/11\] TurboMind é‡ç£…å‡çº§ã€‚åŒ…æ‹¬ï¼šPaged Attentionã€æ›´å¿«çš„ä¸”ä¸å—åºåˆ—æœ€å¤§é•¿åº¦é™åˆ¶çš„ attention kernelã€2+å€å¿«çš„ KV8 kernelsã€Split-K decoding (Flash Decoding) å’Œ æ”¯æŒ sm_75 æ¶æ„çš„ W4A16
- \[2023/09\] TurboMind æ”¯æŒ Qwen-14B
- \[2023/09\] TurboMind æ”¯æŒ InternLM-20B æ¨¡å‹
- \[2023/09\] TurboMind æ”¯æŒ Code Llama æ‰€æœ‰åŠŸèƒ½ï¼šä»£ç ç»­å†™ã€å¡«ç©ºã€å¯¹è¯ã€Pythonä¸“é¡¹ã€‚ç‚¹å‡»[è¿™é‡Œ](./docs/zh_cn/supported_models/codellama.md)é˜…è¯»éƒ¨ç½²æ–¹æ³•
- \[2023/09\] TurboMind æ”¯æŒ Baichuan2-7B
- \[2023/08\] TurboMind æ”¯æŒ flash-attention2
- \[2023/08\] TurboMind æ”¯æŒ Qwen-7Bï¼ŒåŠ¨æ€NTK-RoPEç¼©æ”¾ï¼ŒåŠ¨æ€logNç¼©æ”¾
- \[2023/08\] TurboMind æ”¯æŒ Windows (tp=1)
- \[2023/08\] TurboMind æ”¯æŒ 4-bit æ¨ç†ï¼Œé€Ÿåº¦æ˜¯ FP16 çš„ 2.4 å€ï¼Œæ˜¯ç›®å‰æœ€å¿«çš„å¼€æºå®ç°ã€‚éƒ¨ç½²æ–¹å¼è¯·çœ‹[è¿™é‡Œ](docs/zh_cn/quantization/w4a16.md)
- \[2023/08\] LMDeploy å¼€é€šäº† [HuggingFace Hub](https://huggingface.co/lmdeploy) ï¼Œæä¾›å¼€ç®±å³ç”¨çš„ 4-bit æ¨¡å‹
- \[2023/08\] LMDeploy æ”¯æŒä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•è¿›è¡Œ 4-bit é‡åŒ–
- \[2023/07\] TurboMind æ”¯æŒä½¿ç”¨ GQA çš„ Llama-2 70B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ Llama-2 7B/13B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ InternLM çš„ Tensor Parallel æ¨ç†

</details>
______________________________________________________________________

# ç®€ä»‹

LMDeploy ç”± [MMDeploy](https://github.com/open-mmlab/mmdeploy) å’Œ [MMRazor](https://github.com/open-mmlab/mmrazor) å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚
è¿™ä¸ªå¼ºå¤§çš„å·¥å…·ç®±æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

- **é«˜æ•ˆçš„æ¨ç†**ï¼šLMDeploy å¼€å‘äº† Persistent Batch(å³ Continuous Batch)ï¼ŒBlocked K/V Cacheï¼ŒåŠ¨æ€æ‹†åˆ†å’Œèåˆï¼Œå¼ é‡å¹¶è¡Œï¼Œé«˜æ•ˆçš„è®¡ç®— kernelç­‰é‡è¦ç‰¹æ€§ã€‚æ¨ç†æ€§èƒ½æ˜¯ vLLM çš„ 1.8 å€

- **å¯é çš„é‡åŒ–**ï¼šLMDeploy æ”¯æŒæƒé‡é‡åŒ–å’Œ k/v é‡åŒ–ã€‚4bit æ¨¡å‹æ¨ç†æ•ˆç‡æ˜¯ FP16 ä¸‹çš„ 2.4 å€ã€‚é‡åŒ–æ¨¡å‹çš„å¯é æ€§å·²é€šè¿‡ OpenCompass è¯„æµ‹å¾—åˆ°å……åˆ†éªŒè¯ã€‚

- **ä¾¿æ·çš„æœåŠ¡**ï¼šé€šè¿‡è¯·æ±‚åˆ†å‘æœåŠ¡ï¼ŒLMDeploy æ”¯æŒå¤šæ¨¡å‹åœ¨å¤šæœºã€å¤šå¡ä¸Šçš„æ¨ç†æœåŠ¡ã€‚

- **æœ‰çŠ¶æ€æ¨ç†**ï¼šé€šè¿‡ç¼“å­˜å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­ attention çš„ k/vï¼Œè®°ä½å¯¹è¯å†å²ï¼Œä»è€Œé¿å…é‡å¤å¤„ç†å†å²ä¼šè¯ã€‚æ˜¾è‘—æå‡é•¿æ–‡æœ¬å¤šè½®å¯¹è¯åœºæ™¯ä¸­çš„æ•ˆç‡ã€‚

# æ€§èƒ½

LMDeploy TurboMind å¼•æ“æ‹¥æœ‰å“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨å„ç§è§„æ¨¡çš„æ¨¡å‹ä¸Šï¼Œæ¯ç§’å¤„ç†çš„è¯·æ±‚æ•°æ˜¯ vLLM çš„ 1.36 ~ 1.85 å€ã€‚åœ¨é™æ€æ¨ç†èƒ½åŠ›æ–¹é¢ï¼ŒTurboMind 4bit æ¨¡å‹æ¨ç†é€Ÿåº¦ï¼ˆout token/sï¼‰è¿œé«˜äº FP16/BF16 æ¨ç†ã€‚åœ¨å° batch æ—¶ï¼Œæé«˜åˆ° 2.4 å€ã€‚

![v0 1 0-benchmark](https://github.com/InternLM/lmdeploy/assets/4560679/8e455cf1-a792-4fa8-91a2-75df96a2a5ba)

æ›´å¤šè®¾å¤‡ã€æ›´å¤šè®¡ç®—ç²¾åº¦ã€æ›´å¤šsettingä¸‹çš„çš„æ¨ç† benchmarkï¼Œè¯·å‚è€ƒä»¥ä¸‹é“¾æ¥ï¼š

- [A100](./docs/en/benchmark/a100_fp16.md)
- 4090
- 3090
- 2080

# æ”¯æŒçš„æ¨¡å‹

|       Model        |   Size    |
| :----------------: | :-------: |
|       Llama        | 7B - 65B  |
|       Llama2       | 7B - 70B  |
|      InternLM      | 7B - 20B  |
| InternLM-XComposer |    7B     |
|        QWen        | 7B - 72B  |
|      QWen-VL       |    7B     |
|      Baichuan      | 7B - 13B  |
|     Baichuan2      | 7B - 13B  |
|     Code Llama     | 7B - 34B  |
|      ChatGLM2      |    6B     |
|       Falcon       | 7B - 180B |

LMDeploy æ”¯æŒ 2 ç§æ¨ç†å¼•æ“ï¼š [TurboMind](./docs/zh_cn/inference/turbomind.md) å’Œ [PyTorch](./docs/zh_cn/inference/pytorch.md)ï¼Œå®ƒä»¬ä¾§é‡ä¸åŒã€‚å‰è€…è¿½æ±‚æ¨ç†æ€§èƒ½çš„æè‡´ä¼˜åŒ–ï¼Œåè€…çº¯ç”¨pythonå¼€å‘ï¼Œç€é‡é™ä½å¼€å‘è€…çš„é—¨æ§›ã€‚

å®ƒä»¬åœ¨æ”¯æŒçš„æ¨¡å‹ç±»åˆ«ã€è®¡ç®—ç²¾åº¦æ–¹é¢æœ‰æ‰€å·®åˆ«ã€‚ç”¨æˆ·å¯å‚è€ƒ[è¿™é‡Œ](./docs/zh_cn/supported_models/supported_models.md), æŸ¥é˜…æ¯ä¸ªæ¨ç†å¼•æ“çš„èƒ½åŠ›ï¼Œå¹¶æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„ã€‚

# å¿«é€Ÿå¼€å§‹

## å®‰è£…

ä½¿ç”¨ pip ( python 3.8+) å®‰è£… LMDeployï¼Œæˆ–è€…[æºç å®‰è£…](./docs/zh_cn/build.md)

```shell
pip install lmdeploy
```

## ç¦»çº¿æ‰¹å¤„ç†

```python
import lmdeploy
pipe = lmdeploy.pipeline("internlm/internlm-chat-7b")
response = pipe(["Hi, pls intro yourself", "Shanghai is"])
print(response)
```

å…³äº pipeline çš„æ›´å¤šæ¨ç†å‚æ•°è¯´æ˜ï¼Œè¯·å‚è€ƒ[è¿™é‡Œ](./docs/zh_cn/inference/pipeline.md)

# ç”¨æˆ·æ•™ç¨‹

è¯·é˜…è¯»[å¿«é€Ÿä¸Šæ‰‹](./docs/zh_cn/get_started.md)ç« èŠ‚ï¼Œäº†è§£ LMDeploy çš„åŸºæœ¬ç”¨æ³•ã€‚

ä¸ºäº†å¸®åŠ©ç”¨æˆ·æ›´è¿›ä¸€æ­¥äº†è§£ LMDeployï¼Œæˆ‘ä»¬å‡†å¤‡äº†ç”¨æˆ·æŒ‡å—å’Œè¿›é˜¶æŒ‡å—ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„[æ–‡æ¡£](https://lmdeploy.readthedocs.io/zh-cn/latest/)ï¼š

- ç”¨æˆ·æŒ‡å—
  - [æ¨ç†pipeline](./docs/zh_cn/inference/pipeline.md)
  - [æ¨ç†å¼•æ“ - TurboMind](./docs/zh_cn/inference/turbomind.md)
  - [æ¨ç†å¼•æ“ - PyTorch](./docs/zh_cn/inference/pytorch.md)
  - [æ¨ç†æœåŠ¡](./docs/zh_cn/serving/restful_api.md)
  - [æ¨¡å‹é‡åŒ–](./docs/zh_cn/quantization)
- è¿›é˜¶æŒ‡å—
  - å¢åŠ å¯¹è¯æ¨¡æ¿
  - æ”¯æŒæ–°æ¨¡å‹
  - gemm tuning
  - [é•¿æ–‡æœ¬æ¨ç†](./docs/zh_cn/advance/long_context.md)
  - [å¤šæ¨¡å‹æ¨ç†æœåŠ¡](./docs/zh_cn/serving/proxy_server.md)

## è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ LMDeploy æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚è¯·å‚è€ƒ[è´¡çŒ®æŒ‡å—](.github/CONTRIBUTING.md)æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

## è‡´è°¢

- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [llm-awq](https://github.com/mit-han-lab/llm-awq)
- [vLLM](https://github.com/vllm-project/vllm)
- [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII)

## License

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache 2.0 å¼€æºè®¸å¯è¯](LICENSE)ã€‚
