<div align="center">
  <img src="resources/lmdeploy-logo.png" width="450"/>

[English](README.md) | ç®€ä½“ä¸­æ–‡

</div>

<p align="center">
    ğŸ‘‹ join us on <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> and <a href="https://r.vansin.top/?r=internwx" target="_blank">WeChat</a>
</p>

______________________________________________________________________

## æ›´æ–° ğŸ‰

- \[2023/07\] TurboMind æ”¯æŒä½¿ç”¨ GQA çš„ Llama-2 70B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ Llama-2 7B/13B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ InternLM çš„ Tensor Parallel æ¨ç†

______________________________________________________________________

## ç®€ä»‹

LMDeploy ç”± [MMDeploy](https://github.com/open-mmlab/mmdeploy) å’Œ [MMRazor](https://github.com/open-mmlab/mmrazor) å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚
è¿™ä¸ªå¼ºå¤§çš„å·¥å…·ç®±æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

- **é«˜æ•ˆæ¨ç†å¼•æ“ TurboMind**ï¼šåŸºäº [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)ï¼Œæˆ‘ä»¬å®ç°äº†é«˜æ•ˆæ¨ç†å¼•æ“ TurboMindï¼Œæ”¯æŒ InternLMã€LLaMAã€vicunaç­‰æ¨¡å‹åœ¨ NVIDIA GPU ä¸Šçš„æ¨ç†ã€‚

- **äº¤äº’æ¨ç†æ–¹å¼**ï¼šé€šè¿‡ç¼“å­˜å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­ attention çš„ k/vï¼Œè®°ä½å¯¹è¯å†å²ï¼Œä»è€Œé¿å…é‡å¤å¤„ç†å†å²ä¼šè¯ã€‚

- **å¤š GPU éƒ¨ç½²å’Œé‡åŒ–**ï¼šæˆ‘ä»¬æä¾›äº†å…¨é¢çš„æ¨¡å‹éƒ¨ç½²å’Œé‡åŒ–æ”¯æŒï¼Œå·²åœ¨ä¸åŒè§„æ¨¡ä¸Šå®ŒæˆéªŒè¯ã€‚

- **persistent batch æ¨ç†**ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ‰§è¡Œæ•ˆç‡ã€‚

  ![PersistentBatchInference](https://github.com/InternLM/lmdeploy/assets/67539920/e3876167-0671-44fc-ac52-5a0f9382493e)

## æ€§èƒ½

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬å¯¹æ¯”äº† facebookresearch/llamaã€HuggingFace Transformersã€DeepSpeed åœ¨ 7B æ¨¡å‹ä¸Šçš„tokenç”Ÿæˆçš„é€Ÿåº¦ã€‚

æµ‹è¯•è®¾å¤‡ï¼šNVIDIA A100(80G)

æµ‹è¯•æŒ‡æ ‡ï¼šååé‡ï¼ˆtoken/s)

æµ‹è¯•æ•°æ®ï¼šè¾“å…¥tokenæ•°ä¸º1ï¼Œç”Ÿæˆtokenæ•°ä¸º2048

TurboMind çš„ååé‡è¶…è¿‡ 2000 token/s, æ•´ä½“æ¯” DeepSpeed æå‡çº¦ 5% - 15%ï¼Œæ¯” huggingface transformers æå‡ 2.3 å€

![benchmark](https://user-images.githubusercontent.com/12756472/251422522-e94a3db9-eb16-432a-8d8c-078945e7b99a.png)

## å¿«é€Ÿä¸Šæ‰‹

### å®‰è£…

```shell
conda create -n lmdeploy python=3.10 -y
conda activate lmdeploy
git clone https://github.com/InternLM/lmdeploy.git
cd lmdeploy
pip install -e .
```

### éƒ¨ç½² InternLM

#### è·å– InternLM æ¨¡å‹

```shell
# 1. ä¸‹è½½ InternLM æ¨¡å‹

# Make sure you have git-lfs installed (https://git-lfs.com)
git lfs install
git clone https://huggingface.co/internlm/internlm-chat-7b /path/to/internlm-chat-7b

# if you want to clone without large files â€“ just their pointers
# prepend your git clone with the following env var:
GIT_LFS_SKIP_SMUDGE=1

# 2. è½¬æ¢ä¸º trubomind è¦æ±‚çš„æ ¼å¼ã€‚é»˜è®¤å­˜æ”¾è·¯å¾„ä¸º ./workspace
python3 -m lmdeploy.serve.turbomind.deploy internlm-chat-7b /path/to/internlm-chat-7b

```

#### ä½¿ç”¨ turbomind æ¨ç†

```shell
docker run --gpus all --rm -v $(pwd)/workspace:/workspace -it openmmlab/lmdeploy:latest \
    python3 -m lmdeploy.turbomind.chat /workspace
```

```{note}
turbomind åœ¨ä½¿ç”¨ FP16 ç²¾åº¦æ¨ç† InternLM-7B æ¨¡å‹æ—¶ï¼Œæ˜¾å­˜å¼€é”€è‡³å°‘éœ€è¦ 15.7Gã€‚å»ºè®®ä½¿ç”¨ 3090, V100ï¼ŒA100ç­‰å‹å·çš„æ˜¾å¡
```

#### éƒ¨ç½²æ¨ç†æœåŠ¡

ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š

```shell
bash workspace/service_docker_up.sh
```

ä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨ç†æœåŠ¡è¿›è¡Œå¯¹è¯ï¼š

```shell
python3 -m lmdeploy.serve.client {server_ip_addresss}:33337
```

ä¹Ÿå¯ä»¥é€šè¿‡ WebUI æ–¹å¼æ¥å¯¹è¯ï¼š

```shell
python3 -m lmdeploy.app {server_ip_addresss}:33337
```

![](https://github.com/InternLM/lmdeploy/assets/67539920/08d1e6f2-3767-44d5-8654-c85767cec2ab)

å…¶ä»–æ¨¡å‹çš„éƒ¨ç½²æ–¹å¼ï¼Œæ¯”å¦‚ LLaMAï¼ŒLLaMA-2ï¼Œvicunaç­‰ç­‰ï¼Œè¯·å‚è€ƒ[è¿™é‡Œ](docs/zh_cn/serving.md)

### åŸºäº PyTorch çš„æ¨ç†

ä½ å¿…é¡»ç¡®ä¿ç¯å¢ƒä¸­æœ‰å®‰è£… deepspeedï¼š

```
pip install deepspeed
```

#### å•ä¸ª GPU

```shell
python3 -m lmdeploy.pytorch.chat $NAME_OR_PATH_TO_HF_MODEL\
    --max_new_tokens 64 \
    --temperture 0.8 \
    --top_p 0.95 \
    --seed 0
```

#### ä½¿ç”¨ DeepSpeed å®ç°å¼ é‡å¹¶è¡Œ

```shell
deepspeed --module --num_gpus 2 lmdeploy.pytorch.chat \
    $NAME_OR_PATH_TO_HF_MODEL \
    --max_new_tokens 64 \
    --temperture 0.8 \
    --top_p 0.95 \
    --seed 0
```

## é‡åŒ–éƒ¨ç½²

åœ¨ fp16 æ¨¡å¼ä¸‹ï¼Œå¯ä»¥å¼€å¯ kv_cache int8 é‡åŒ–ï¼Œå•å¡å¯æœåŠ¡æ›´å¤šç”¨æˆ·ã€‚
é¦–å…ˆæ‰§è¡Œé‡åŒ–è„šæœ¬ï¼Œé‡åŒ–å‚æ•°å­˜æ”¾åˆ° `deploy.py` è½¬æ¢çš„ `workspace/triton_models/weights` ç›®å½•ä¸‹ã€‚

```
python3 -m lmdeploy.lite.apis.kv_qparams \
  --model $HF_MODEL \
  --output_dir $DEPLOY_WEIGHT_DIR \
  --symmetry True \ # å¯¹ç§°é‡åŒ–æˆ–éå¯¹ç§°é‡åŒ–ï¼Œé»˜è®¤ä¸º True
  --offload  False \ # å°†æ¨¡å‹æ”¾åœ¨ CPUï¼Œåªåœ¨æ¨ç†æ—¶åŠ è½½éƒ¨åˆ†æ¨¡å—åˆ° GPUï¼Œé»˜è®¤ä¸º False
  --num_tp 1  \  # Tensor å¹¶è¡Œä½¿ç”¨çš„ GPU æ•°ï¼Œå’Œ deploy.py ä¿æŒä¸€è‡´
```

ç„¶åè°ƒæ•´ `workspace/triton_models/weights/config.ini`

- `use_context_fmha` æ”¹ä¸º 0ï¼Œè¡¨ç¤ºå…³é—­
- `quant_policy` è®¾ç½®ä¸º 4ã€‚æ­¤å‚æ•°é»˜è®¤ä¸º 0ï¼Œè¡¨ç¤ºä¸å¼€å¯

è¿™é‡Œæ˜¯[é‡åŒ–æµ‹è¯•ç»“æœ](./docs/zh_cn/quantization.md)ã€‚

## è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ LMDeploy æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚è¯·å‚è€ƒ[è´¡çŒ®æŒ‡å—](.github/CONTRIBUTING.md)æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

## è‡´è°¢

- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)

## License

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache 2.0 å¼€æºè®¸å¯è¯](LICENSE)ã€‚
