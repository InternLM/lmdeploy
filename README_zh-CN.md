<div align="center">
  <img src="resources/lmdeploy-logo.svg" width="450"/>

[![docs](https://img.shields.io/badge/docs-latest-blue)](https://lmdeploy-zh-cn.readthedocs.io/zh_CN/latest/)
[![badge](https://github.com/InternLM/lmdeploy/workflows/lint/badge.svg)](https://github.com/InternLM/lmdeploy/actions)
[![PyPI](https://img.shields.io/pypi/v/lmdeploy)](https://pypi.org/project/lmdeploy)
[![license](https://img.shields.io/github/license/InternLM/lmdeploy.svg)](https://github.com/InternLM/lmdeploy/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)
[![open issues](https://img.shields.io/github/issues-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)

[English](README.md) | ç®€ä½“ä¸­æ–‡

</div>

<p align="center">
    ğŸ‘‹ join us on <a href="https://twitter.com/intern_lm" target="_blank">Twitter</a>, <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> and <a href="https://r.vansin.top/?r=internwx" target="_blank">WeChat</a>
</p>

______________________________________________________________________

## æ›´æ–° ğŸ‰

- \[2023/09\] TurboMind æ”¯æŒ Qwen-14B
- \[2023/09\] TurboMind æ”¯æŒ InternLM-20B æ¨¡å‹
- \[2023/09\] TurboMind æ”¯æŒ Code Llama æ‰€æœ‰åŠŸèƒ½ï¼šä»£ç ç»­å†™ã€å¡«ç©ºã€å¯¹è¯ã€Pythonä¸“é¡¹ã€‚ç‚¹å‡»[è¿™é‡Œ](./docs/zh_cn/supported_models/codellama.md)é˜…è¯»éƒ¨ç½²æ–¹æ³•
- \[2023/09\] TurboMind æ”¯æŒ Baichuan2-7B
- \[2023/08\] TurboMind æ”¯æŒ flash-attention2
- \[2023/08\] TurboMind æ”¯æŒ Qwen-7Bï¼ŒåŠ¨æ€NTK-RoPEç¼©æ”¾ï¼ŒåŠ¨æ€logNç¼©æ”¾
- \[2023/08\] TurboMind æ”¯æŒ Windows (tp=1)
- \[2023/08\] TurboMind æ”¯æŒ 4-bit æ¨ç†ï¼Œé€Ÿåº¦æ˜¯ FP16 çš„ 2.4 å€ï¼Œæ˜¯ç›®å‰æœ€å¿«çš„å¼€æºå®ç°ğŸš€ã€‚éƒ¨ç½²æ–¹å¼è¯·çœ‹[è¿™é‡Œ](./docs/zh_cn/w4a16.md)
- \[2023/08\] LMDeploy å¼€é€šäº† [HuggingFace Hub](https://huggingface.co/lmdeploy) ï¼Œæä¾›å¼€ç®±å³ç”¨çš„ 4-bit æ¨¡å‹
- \[2023/08\] LMDeploy æ”¯æŒä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•è¿›è¡Œ 4-bit é‡åŒ–
- \[2023/07\] TurboMind æ”¯æŒä½¿ç”¨ GQA çš„ Llama-2 70B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ Llama-2 7B/13B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ InternLM çš„ Tensor Parallel æ¨ç†

______________________________________________________________________

## ç®€ä»‹

LMDeploy ç”± [MMDeploy](https://github.com/open-mmlab/mmdeploy) å’Œ [MMRazor](https://github.com/open-mmlab/mmrazor) å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚
è¿™ä¸ªå¼ºå¤§çš„å·¥å…·ç®±æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

- **é«˜æ•ˆæ¨ç†å¼•æ“ TurboMind**ï¼šåŸºäº [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)ï¼Œæˆ‘ä»¬å®ç°äº†é«˜æ•ˆæ¨ç†å¼•æ“ TurboMindï¼Œæ”¯æŒ InternLMã€LLaMAã€vicunaç­‰æ¨¡å‹åœ¨ NVIDIA GPU ä¸Šçš„æ¨ç†ã€‚

- **äº¤äº’æ¨ç†æ–¹å¼**ï¼šé€šè¿‡ç¼“å­˜å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­ attention çš„ k/vï¼Œè®°ä½å¯¹è¯å†å²ï¼Œä»è€Œé¿å…é‡å¤å¤„ç†å†å²ä¼šè¯ã€‚

- **å¤š GPU éƒ¨ç½²å’Œé‡åŒ–**ï¼šæˆ‘ä»¬æä¾›äº†å…¨é¢çš„æ¨¡å‹éƒ¨ç½²å’Œé‡åŒ–æ”¯æŒï¼Œå·²åœ¨ä¸åŒè§„æ¨¡ä¸Šå®ŒæˆéªŒè¯ã€‚

- **persistent batch æ¨ç†**ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ‰§è¡Œæ•ˆç‡ã€‚

  ![PersistentBatchInference](https://github.com/InternLM/lmdeploy/assets/67539920/e3876167-0671-44fc-ac52-5a0f9382493e)

## æ”¯æŒçš„æ¨¡å‹

`LMDeploy` æ”¯æŒ `TurboMind` å’Œ `Pytorch` ä¸¤ç§æ¨ç†åç«¯

### TurboMind

> **Note**<br />
> W4A16 æ¨ç†éœ€è¦ Ampere åŠä»¥ä¸Šæ¶æ„çš„ Nvidia GPU

|     æ¨¡å‹     | æ¨¡å‹å¹¶è¡Œ | FP16 | KV INT8 | W4A16 | W8A8 |
| :----------: | :------: | :--: | :-----: | :---: | :--: |
|    Llama     |   Yes    | Yes  |   Yes   |  Yes  |  No  |
|    Llama2    |   Yes    | Yes  |   Yes   |  Yes  |  No  |
| InternLM-7B  |   Yes    | Yes  |   Yes   |  Yes  |  No  |
| InternLM-20B |   Yes    | Yes  |   Yes   |  Yes  |  No  |
|   QWen-7B    |   Yes    | Yes  |   Yes   |  No   |  No  |
|   QWen-14B   |   Yes    | Yes  |   Yes   |  No   |  No  |
| Baichuan-7B  |   Yes    | Yes  |   Yes   |  Yes  |  No  |
| Baichuan2-7B |   Yes    | Yes  |   No    |  No   |  No  |
|  Code Llama  |   Yes    | Yes  |   No    |  No   |  No  |

### Pytorch

|    æ¨¡å‹     | æ¨¡å‹å¹¶è¡Œ | FP16 | KV INT8 | W4A16 | W8A8 |
| :---------: | :------: | :--: | :-----: | :---: | :--: |
|    Llama    |   Yes    | Yes  |   No    |  No   |  No  |
|   Llama2    |   Yes    | Yes  |   No    |  No   |  No  |
| InternLM-7B |   Yes    | Yes  |   No    |  No   |  No  |

## æ€§èƒ½

**åœºæ™¯ä¸€**: å›ºå®šçš„è¾“å…¥ã€è¾“å‡ºtokenæ•°ï¼ˆ1,2048ï¼‰ï¼Œæµ‹è¯• output token throughput

**åœºæ™¯äºŒ**: ä½¿ç”¨çœŸå®æ•°æ®ï¼Œæµ‹è¯• request throughput

æµ‹è¯•é…ç½®ï¼šLLaMA-7B, NVIDIA A100(80G)

TurboMind çš„ output token throughput è¶…è¿‡ 2000 token/s, æ•´ä½“æ¯” DeepSpeed æå‡çº¦ 5% - 15%ï¼Œæ¯” huggingface transformers æå‡ 2.3 å€
åœ¨ request throughput æŒ‡æ ‡ä¸Šï¼ŒTurboMind çš„æ•ˆç‡æ¯” vLLM é«˜ 30%

![benchmark](https://github.com/InternLM/lmdeploy/assets/4560679/7775c518-608e-4e5b-be73-7645a444e774)

## å¿«é€Ÿä¸Šæ‰‹

### å®‰è£…

ä½¿ç”¨ pip ( python 3.8+) å®‰è£… LMDeployï¼Œæˆ–è€…[æºç å®‰è£…](./docs/zh_cn/build.md)

```shell
pip install lmdeploy
```

### éƒ¨ç½² InternLM

#### è·å– InternLM æ¨¡å‹

```shell
# 1. ä¸‹è½½ InternLM æ¨¡å‹

# Make sure you have git-lfs installed (https://git-lfs.com)
git lfs install
git clone https://huggingface.co/internlm/internlm-chat-7b-v1_1 /path/to/internlm-chat-7b

# if you want to clone without large files â€“ just their pointers
# prepend your git clone with the following env var:
GIT_LFS_SKIP_SMUDGE=1

# 2. è½¬æ¢ä¸º trubomind è¦æ±‚çš„æ ¼å¼ã€‚é»˜è®¤å­˜æ”¾è·¯å¾„ä¸º ./workspace
python3 -m lmdeploy.serve.turbomind.deploy internlm-chat-7b /path/to/internlm-chat-7b

```

#### ä½¿ç”¨ turbomind æ¨ç†

```shell
python3 -m lmdeploy.turbomind.chat ./workspace
```

> **Note**<br />
> turbomind åœ¨ä½¿ç”¨ FP16 ç²¾åº¦æ¨ç† InternLM-7B æ¨¡å‹æ—¶ï¼Œæ˜¾å­˜å¼€é”€è‡³å°‘éœ€è¦ 15.7Gã€‚å»ºè®®ä½¿ç”¨ 3090, V100ï¼ŒA100ç­‰å‹å·çš„æ˜¾å¡ã€‚<br />
> å…³é—­æ˜¾å¡çš„ ECC å¯ä»¥è…¾å‡º 10% æ˜¾å­˜ï¼Œæ‰§è¡Œ `sudo nvidia-smi --ecc-config=0` é‡å¯ç³»ç»Ÿç”Ÿæ•ˆã€‚

> **Note**<br />
> ä½¿ç”¨ Tensor å¹¶å‘å¯ä»¥åˆ©ç”¨å¤šå¼  GPU è¿›è¡Œæ¨ç†ã€‚åœ¨ `chat` æ—¶æ·»åŠ å‚æ•° `--tp=<num_gpu>` å¯ä»¥å¯åŠ¨è¿è¡Œæ—¶ TPã€‚

#### å¯åŠ¨ gradio server

```shell
python3 -m lmdeploy.serve.gradio.app ./workspace
```

![](https://github.com/InternLM/lmdeploy/assets/67539920/08d1e6f2-3767-44d5-8654-c85767cec2ab)

#### é€šè¿‡ Restful API éƒ¨ç½²æœåŠ¡

ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š

```shell
python3 -m lmdeploy.serve.openai.api_server ./workspace server_ip server_port --instance_num 32 --tp 1
```

ä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨ç†æœåŠ¡è¿›è¡Œå¯¹è¯ï¼š

```shell
# restful_api_url is what printed in api_server.py, e.g. http://localhost:23333
python -m lmdeploy.serve.openai.api_client restful_api_url
```

ä¹Ÿå¯ä»¥é€šè¿‡ WebUI æ–¹å¼æ¥å¯¹è¯ï¼š

```shell
# restful_api_url is what printed in api_server.py, e.g. http://localhost:23333
# server_ip and server_port here are for gradio ui
# example: python -m lmdeploy.serve.gradio.app http://localhost:23333 localhost 6006 --restful_api True
python -m lmdeploy.serve.gradio.app restful_api_url server_ip --restful_api True
```

æ›´å¤šè¯¦æƒ…å¯ä»¥æŸ¥é˜… [restful_api.md](docs/zh_cn/restful_api.md)ã€‚

#### é€šè¿‡å®¹å™¨éƒ¨ç½²æ¨ç†æœåŠ¡

ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š

```shell
bash workspace/service_docker_up.sh
```

ä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨ç†æœåŠ¡è¿›è¡Œå¯¹è¯ï¼š

```shell
python3 -m lmdeploy.serve.client {server_ip_addresss}:33337
```

ä¹Ÿå¯ä»¥é€šè¿‡ WebUI æ–¹å¼æ¥å¯¹è¯ï¼š

```shell
python3 -m lmdeploy.serve.gradio.app {server_ip_addresss}:33337
```

å…¶ä»–æ¨¡å‹çš„éƒ¨ç½²æ–¹å¼ï¼Œæ¯”å¦‚ LLaMAï¼ŒLLaMA-2ï¼Œvicunaç­‰ç­‰ï¼Œè¯·å‚è€ƒ[è¿™é‡Œ](docs/zh_cn/serving.md)

### åŸºäº PyTorch çš„æ¨ç†

ä½ å¿…é¡»ç¡®ä¿ç¯å¢ƒä¸­æœ‰å®‰è£… deepspeedï¼š

```
pip install deepspeed
```

#### å•ä¸ª GPU

```shell
python3 -m lmdeploy.pytorch.chat $NAME_OR_PATH_TO_HF_MODEL\
    --max_new_tokens 64 \
    --temperture 0.8 \
    --top_p 0.95 \
    --seed 0
```

#### ä½¿ç”¨ DeepSpeed å®ç°å¼ é‡å¹¶è¡Œ

```shell
deepspeed --module --num_gpus 2 lmdeploy.pytorch.chat \
    $NAME_OR_PATH_TO_HF_MODEL \
    --max_new_tokens 64 \
    --temperture 0.8 \
    --top_p 0.95 \
    --seed 0
```

## é‡åŒ–éƒ¨ç½²

#### æƒé‡ INT4 é‡åŒ–

LMDeploy ä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•å¯¹æ¨¡å‹æƒé‡è¿›è¡Œé‡åŒ–

[ç‚¹å‡»è¿™é‡Œ](./docs/zh_cn/w4a16.md) æŸ¥çœ‹ weight int4 ç”¨æ³•æµ‹è¯•ç»“æœã€‚

#### KV Cache INT8 é‡åŒ–

[ç‚¹å‡»è¿™é‡Œ](./docs/zh_cn/kv_int8.md) æŸ¥çœ‹ kv int8 ä½¿ç”¨æ–¹æ³•ã€å®ç°å…¬å¼å’Œæµ‹è¯•ç»“æœã€‚

> **Warning**<br />
> é‡åŒ–éƒ¨ç½²ä¸æ”¯æŒè¿è¡Œæ—¶ Tensor å¹¶å‘ã€‚å¦‚æœå¸Œæœ›ä½¿ç”¨ Tensor å¹¶å‘ï¼Œéœ€è¦åœ¨ deploy æ—¶é…ç½® tp å‚æ•°ã€‚

## è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ LMDeploy æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚è¯·å‚è€ƒ[è´¡çŒ®æŒ‡å—](.github/CONTRIBUTING.md)æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

## è‡´è°¢

- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [llm-awq](https://github.com/mit-han-lab/llm-awq)

## License

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache 2.0 å¼€æºè®¸å¯è¯](LICENSE)ã€‚
