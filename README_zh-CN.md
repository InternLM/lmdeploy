<div align="center">
  <img src="resources/lmdeploy-logo.svg" width="450"/>

[![docs](https://img.shields.io/badge/docs-latest-blue)](https://lmdeploy-zh-cn.readthedocs.io/zh_CN/latest/)
[![badge](https://github.com/InternLM/lmdeploy/workflows/lint/badge.svg)](https://github.com/InternLM/lmdeploy/actions)
[![PyPI](https://img.shields.io/pypi/v/lmdeploy)](https://pypi.org/project/lmdeploy)
[![license](https://img.shields.io/github/license/InternLM/lmdeploy.svg)](https://github.com/InternLM/lmdeploy/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)
[![open issues](https://img.shields.io/github/issues-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)

[English](README.md) | ç®€ä½“ä¸­æ–‡

</div>

<p align="center">
    ğŸ‘‹ join us on <a href="https://twitter.com/intern_lm" target="_blank">Twitter</a>, <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> and <a href="https://r.vansin.top/?r=internwx" target="_blank">WeChat</a>
</p>

______________________________________________________________________

## æœ€æ–°è¿›å±• ğŸ‰

- \[2023/12\] Turbomind æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€‚[Gradio Demo](./examples/vl/README.md)
- \[2023/11\] Turbomind æ”¯æŒç›´æ¥è¯»å– Huggingface æ¨¡å‹ã€‚ç‚¹å‡»[è¿™é‡Œ](./docs/en/load_hf.md)æŸ¥çœ‹ä½¿ç”¨æ–¹æ³•
- \[2023/11\] TurboMind é‡ç£…å‡çº§ã€‚åŒ…æ‹¬ï¼šPaged Attentionã€æ›´å¿«çš„ä¸”ä¸å—åºåˆ—æœ€å¤§é•¿åº¦é™åˆ¶çš„ attention kernelã€2+å€å¿«çš„ KV8 kernelsã€Split-K decoding (Flash Decoding) å’Œ æ”¯æŒ sm_75 æ¶æ„çš„ W4A16
- \[2023/09\] TurboMind æ”¯æŒ Qwen-14B
- \[2023/09\] TurboMind æ”¯æŒ InternLM-20B æ¨¡å‹
- \[2023/09\] TurboMind æ”¯æŒ Code Llama æ‰€æœ‰åŠŸèƒ½ï¼šä»£ç ç»­å†™ã€å¡«ç©ºã€å¯¹è¯ã€Pythonä¸“é¡¹ã€‚ç‚¹å‡»[è¿™é‡Œ](./docs/zh_cn/supported_models/codellama.md)é˜…è¯»éƒ¨ç½²æ–¹æ³•
- \[2023/09\] TurboMind æ”¯æŒ Baichuan2-7B
- \[2023/08\] TurboMind æ”¯æŒ flash-attention2
- \[2023/08\] TurboMind æ”¯æŒ Qwen-7Bï¼ŒåŠ¨æ€NTK-RoPEç¼©æ”¾ï¼ŒåŠ¨æ€logNç¼©æ”¾
- \[2023/08\] TurboMind æ”¯æŒ Windows (tp=1)
- \[2023/08\] TurboMind æ”¯æŒ 4-bit æ¨ç†ï¼Œé€Ÿåº¦æ˜¯ FP16 çš„ 2.4 å€ï¼Œæ˜¯ç›®å‰æœ€å¿«çš„å¼€æºå®ç°ğŸš€ã€‚éƒ¨ç½²æ–¹å¼è¯·çœ‹[è¿™é‡Œ](./docs/zh_cn/w4a16.md)
- \[2023/08\] LMDeploy å¼€é€šäº† [HuggingFace Hub](https://huggingface.co/lmdeploy) ï¼Œæä¾›å¼€ç®±å³ç”¨çš„ 4-bit æ¨¡å‹
- \[2023/08\] LMDeploy æ”¯æŒä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•è¿›è¡Œ 4-bit é‡åŒ–
- \[2023/07\] TurboMind æ”¯æŒä½¿ç”¨ GQA çš„ Llama-2 70B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ Llama-2 7B/13B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ InternLM çš„ Tensor Parallel æ¨ç†

______________________________________________________________________

# ç®€ä»‹

LMDeploy ç”± [MMDeploy](https://github.com/open-mmlab/mmdeploy) å’Œ [MMRazor](https://github.com/open-mmlab/mmrazor) å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚
è¿™ä¸ªå¼ºå¤§çš„å·¥å…·ç®±æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

- **é«˜æ•ˆæ¨ç†å¼•æ“ TurboMind**ï¼šæ”¯æŒ Persistent Batch(å³ Continuous Batch), Blocked K/V Cache, é«˜æ•ˆçš„è®¡ç®— kernelï¼ŒDynamic Split&Fuse ç­‰é‡è¦ç‰¹æ€§ã€‚

- **æœ‰çŠ¶æ€æ¨ç†**ï¼šé€šè¿‡ç¼“å­˜å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­ attention çš„ k/vï¼Œè®°ä½å¯¹è¯å†å²ï¼Œä»è€Œé¿å…é‡å¤å¤„ç†å†å²ä¼šè¯ã€‚æ˜¾è‘—æå‡é•¿æ–‡æœ¬å¤šè½®å¯¹è¯åœºæ™¯ä¸­çš„æ•ˆç‡ã€‚

- **å¤š GPU éƒ¨ç½²å’Œé‡åŒ–**ï¼šæˆ‘ä»¬æä¾›äº†å…¨é¢çš„æ¨¡å‹éƒ¨ç½²å’Œé‡åŒ–æ”¯æŒï¼Œå·²åœ¨ä¸åŒè§„æ¨¡ä¸Šå®ŒæˆéªŒè¯ã€‚

# æ€§èƒ½

## è¯·æ±‚å¤„ç†æ€§èƒ½(req/s)

## é™æ€æ¨ç†æ€§èƒ½(out tok/s)

æ›´å¤šè®¾å¤‡ã€æ›´å¤šè®¡ç®—ç²¾åº¦çš„æ¨ç† benchmarkï¼Œè¯·é˜…è¯»ä»¥ä¸‹é“¾æ¥ï¼š

- [Geforce 2080](<>)
- [Geforce RTX 3090](<>)
- [Geforce RTX 4090](<>)

# æ”¯æŒçš„æ¨¡å‹

`LMDeploy` æ”¯æŒ 2 ç§æ¨ç†å¼•æ“ï¼š `TurboMind` å’Œ `PyTorch`ï¼Œå®ƒä»¬ä¾§é‡ä¸åŒã€‚å‰è€…è¿½æ±‚æ¨ç†æ€§èƒ½çš„æè‡´ä¼˜åŒ–ï¼Œåè€…çº¯ç”¨pythonå¼€å‘ï¼Œç€é‡é™ä½å¼€å‘è€…çš„é—¨æ§›ã€‚

ä¸åŒçš„æ¨ç†å¼•æ“åœ¨æ”¯æŒçš„æ¨¡å‹ç±»åˆ«ã€è®¡ç®—ç²¾åº¦æ–¹é¢æœ‰æ‰€å·®åˆ«ã€‚ç”¨æˆ·å¯æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„ã€‚æœ‰å…³ä¸¤ä¸ªæ¨ç†å¼•æ“çš„æ¶æ„ï¼Œåœ¨[æ­¤å¤„](<>)å¯ä»¥æ‰¾åˆ°ã€‚

## TurboMind æ”¯æŒçš„æ¨¡å‹

|        æ¨¡å‹        | æ¨¡å‹è§„æ¨¡ | FP16/BF16 | KV INT8 | W4A16 |
| :----------------: | :------: | :-------: | :-----: | :---: |
|       Llama        | 7B - 65B |    Yes    |   Yes   |  Yes  |
|       Llama2       | 7B - 70B |    Yes    |   Yes   |  Yes  |
|      InternLM      | 7B - 20B |    Yes    |   Yes   |  Yes  |
| InternLM-XComposer |    7B    |    Yes    |   Yes   |  Yes  |
|        QWen        | 7B - 72B |    Yes    |   Yes   |  Yes  |
|      QWen-VL       |    7B    |    Yes    |   Yes   |  Yes  |
|      Baichuan      |    7B    |    Yes    |   Yes   |  Yes  |
|     Baichuan2      |    7B    |    Yes    |   Yes   |  Yes  |
|     Code Llama     | 7B - 34B |    Yes    |   No    |  No   |

### PyTorch æ”¯æŒçš„æ¨¡å‹

|   æ¨¡å‹    | æ¨¡å‹è§„æ¨¡  | FP16/BF16 | KV INT8 | W8A8 |
| :-------: | :-------: | :-------: | :-----: | :--: |
|   Llama   | 7B - 65B  |    Yes    |   No    | Yes  |
|  Llama2   | 7B - 70B  |    Yes    |   No    | Yes  |
| InternLM  | 7B - 20B  |    Yes    |   No    | Yes  |
| Baichuan2 | 7B - 13B  |    Yes    |   No    | Yes  |
| ChatGLM2  |    6B     |    Yes    |   No    | Yes  |
|  Falcon   | 7B - 180B |    Yes    |   No    | Yes  |

# å¿«é€Ÿä¸Šæ‰‹

LMDeployæä¾›äº†å¿«é€Ÿå®‰è£…ã€æ¨¡å‹é‡åŒ–ã€ç¦»çº¿æ‰¹å¤„ç†ã€åœ¨çº¿æ¨ç†æœåŠ¡ç­‰åŠŸèƒ½ã€‚æ¯ä¸ªåŠŸèƒ½åªéœ€ç®€å•çš„å‡ è¡Œä»£ç æˆ–è€…å‘½ä»¤å°±å¯ä»¥å®Œæˆã€‚

<!-- toc -->

- [å®‰è£…](#å®‰è£…)
- [ç¦»çº¿æ‰¹å¤„ç†](#ç¦»çº¿æ‰¹å¤„ç†)
- [æ¨ç†æœåŠ¡](#æ¨ç†æœåŠ¡)
- [æ¨¡å‹é‡åŒ–](#æ¨¡å‹é‡åŒ–)
- [å¥½ç”¨çš„å·¥å…·](#å¥½ç”¨çš„å·¥å…·)

<!-- tocstop -->

## å®‰è£…

ä½¿ç”¨ pip ( python 3.8+) å®‰è£… LMDeployï¼Œæˆ–è€…[æºç å®‰è£…](./docs/zh_cn/build.md)

```shell
pip install lmdeploy
```

## ç¦»çº¿æ‰¹å¤„ç†

```shell
import lmdeploy
pipe = lmdeploy.pipeline("InternLM/internlm-chat-7b", tp=1)
response = pipe(["Hi, pls intro yourself", "Shanghai is"])
print(response)
```

æ”¯æŒå¤šå¡å¹¶è¡Œå¤„ç†ï¼Œåªç”¨ä¿®æ”¹`tp`å‚æ•°ã€‚å…³äº pipeline çš„æ›´å¤šæ¨ç†å‚æ•°è¯´æ˜ï¼Œè¯·å‚è€ƒ[è¿™é‡Œ](<>)

## æ¨ç†æœåŠ¡

LMDeploy `api_server` æ”¯æŒæŠŠæ¨¡å‹ä¸€é”®å°è£…ä¸ºæœåŠ¡ï¼Œå¯¹å¤–æä¾›çš„ RESTful API å…¼å®¹ openai çš„æ¥å£ã€‚ä»¥ä¸‹ä¸ºæœåŠ¡å¯åŠ¨å’Œè¯·æ±‚å¤„ç†çš„ç¤ºä¾‹ï¼š

```shell
# å¯åŠ¨æœåŠ¡
lmdeploy serve api_server internlm/internlm-chat-7b --server-port 8080 --tp 1
# é€šè¿‡å®¢æˆ·ç«¯ï¼Œå‘é€è¯·æ±‚å’Œæ¥æ”¶ç»“æœ
lmdeploy serve api_client http://0.0.0.0:8080
```

åœ¨ä¸Šè¿°ä¾‹å­ä¸­ï¼ŒæœåŠ¡å¯åŠ¨åï¼Œåœ¨æµè§ˆå™¨è¾“å…¥ `http://0.0.0.0:8080`ï¼Œå¯åœ¨çº¿é˜…è¯»å’Œè¯•ç”¨ `api_server` çš„å„æ¥å£ï¼Œä¹Ÿå¯ç›´æ¥æŸ¥é˜…[æ–‡æ¡£](<>)ï¼Œäº†è§£å„æ¥å£çš„å®šä¹‰å’Œä½¿ç”¨æ–¹æ³•ã€‚

## æ¨¡å‹é‡åŒ–

### æƒé‡ INT4 é‡åŒ–

LMDeploy ä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•å¯¹æ¨¡å‹æƒé‡è¿›è¡Œé‡åŒ–ã€‚

åªç”¨ä¸¤è¡Œå‘½ä»¤ï¼Œå°±å¯ä»¥æŠŠä¸€ä¸ª LLM æ¨¡å‹æƒé‡é‡åŒ–ä¸º 4bitï¼Œå¹¶åœ¨æ§åˆ¶å°ä¸æ¨¡å‹è¿›è¡Œäº¤äº’å¼å¯¹è¯ã€‚

```shell
lmdeploy lite auto_awq internlm/internlm-chat-7b --work-dir ./internlm-chat-7b-4bit
lmdeploy chat turbomind ./internlm-chat-7b-4bit --model-format awq --group-size 128
```

LMDeploy 4bit é‡åŒ–å’Œæ¨ç†æ”¯æŒçš„æ˜¾å¡åŒ…æ‹¬ï¼š

- å›¾çµæ¶æ„ï¼ˆsm75ï¼‰ï¼š20ç³»åˆ—ã€T4
- å®‰åŸ¹æ¶æ„ï¼ˆsm80,sm86ï¼‰ï¼š30ç³»åˆ—ã€A10ã€A16ã€A30ã€A100
- Ada Lovelaceæ¶æ„ï¼ˆsm90ï¼‰ï¼š40 ç³»åˆ—

é‡åŒ–æ¨¡å‹åœ¨å„å‹å·æ˜¾å¡ä¸Šçš„æ¨ç†é€Ÿåº¦å¯ä»¥ä»[è¿™é‡Œ](./docs/zh_cn/w4a16.md)æ‰¾åˆ°ã€‚

### KV Cache INT8 é‡åŒ–

[ç‚¹å‡»è¿™é‡Œ](./docs/zh_cn/kv_int8.md) æŸ¥çœ‹ kv int8 ä½¿ç”¨æ–¹æ³•ã€å®ç°å…¬å¼å’Œæµ‹è¯•ç»“æœã€‚

### W8A8 é‡åŒ–

## å¥½ç”¨çš„å·¥å…·

LMDeploy CLI æä¾›äº†å¦‚ä¸‹ä¾¿æ·çš„å·¥å…·ï¼Œæ–¹ä¾¿ç”¨æˆ·å¿«é€Ÿä½“éªŒæ¨¡å‹å¯¹è¯æ•ˆæœ

### æ§åˆ¶å°äº¤äº’å¼å¯¹è¯

```shell
lmdeploy chat turbomind internlm/internlm-chat-7b
```

***è´´ä¸€å¼ å›¾***

### WebUI äº¤äº’å¼å¯¹è¯

LMDeploy ä½¿ç”¨ gradio å¼€å‘äº†åœ¨çº¿å¯¹è¯ demoã€‚

```shell
# å®‰è£…ä¾èµ–
pip install lmdeploy[serve]
# å¯åŠ¨
lmdeploy serve gradio internlm/internlm-chat-7b --model-name internlm-chat-7b
```

![](https://github.com/InternLM/lmdeploy/assets/67539920/08d1e6f2-3767-44d5-8654-c85767cec2ab)

## è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ LMDeploy æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚è¯·å‚è€ƒ[è´¡çŒ®æŒ‡å—](.github/CONTRIBUTING.md)æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

## è‡´è°¢

- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [llm-awq](https://github.com/mit-han-lab/llm-awq)

## License

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache 2.0 å¼€æºè®¸å¯è¯](LICENSE)ã€‚
