<div align="center">
  <img src="docs/en/_static/image/lmdeploy-logo.svg" width="450"/>

[![PyPI](https://img.shields.io/pypi/v/lmdeploy)](https://pypi.org/project/lmdeploy)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lmdeploy)
[![license](https://img.shields.io/github/license/InternLM/lmdeploy.svg)](https://github.com/InternLM/lmdeploy/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)
[![open issues](https://img.shields.io/github/issues-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)

[ğŸ“˜Documentation](https://lmdeploy.readthedocs.io/zh-cn/latest/) |
[ğŸ› ï¸Quick Start](https://lmdeploy.readthedocs.io/zh-cn/latest/get_started.html) |
[ğŸ¤”Reporting Issues](https://github.com/InternLM/lmdeploy/issues/new/choose)

[English](README.md) | ç®€ä½“ä¸­æ–‡

ğŸ‘‹ join us on [![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=wechat&label=WeChat)](https://r.vansin.top/?r=internwx)
[![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=twitter&label=Twitter)](https://twitter.com/intern_lm)
[![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=discord&label=Discord)](https://discord.gg/xa29JuW87d)

</div>

______________________________________________________________________

## æœ€æ–°è¿›å±• ğŸ‰

<details open>
<summary><b>2024</b></summary>

- \[2024/04\] æ”¯æŒ Llama3 å’Œ InternVL v1.1, v1.2ï¼ŒMiniGeminiï¼ŒInternLM-XComposer2 ç­‰ VLM æ¨¡å‹
- \[2024/04\] TurboMind æ”¯æŒ kv cache int4/int8 åœ¨çº¿é‡åŒ–å’Œæ¨ç†ï¼Œé€‚ç”¨å·²æ”¯æŒçš„æ‰€æœ‰å‹å·æ˜¾å¡ã€‚è¯¦æƒ…è¯·å‚è€ƒ[è¿™é‡Œ](docs/zh_cn/quantization/kv_quant.md)
- \[2024/04\] TurboMind å¼•æ“å‡çº§ï¼Œä¼˜åŒ– GQA æ¨ç†ã€‚[internlm2-20b](https://huggingface.co/internlm/internlm2-20b) æ¨ç†é€Ÿåº¦è¾¾ 16+ RPSï¼Œçº¦æ˜¯ vLLM çš„ 1.8 å€
- \[2024/04\] æ”¯æŒ Qwen1.5-MOE å’Œ dbrx.
- \[2024/03\] æ”¯æŒ DeepSeek-VL çš„ç¦»çº¿æ¨ç† pipeline å’Œæ¨ç†æœåŠ¡
- \[2024/03\] æ”¯æŒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç¦»çº¿æ¨ç† pipeline å’Œæ¨ç†æœåŠ¡
- \[2024/02\] æ”¯æŒ Qwen 1.5ã€Gemmaã€Mistralã€Mixtralã€Deepseek-MOE ç­‰æ¨¡å‹
- \[2024/01\] [OpenAOE](https://github.com/InternLM/OpenAOE) å‘å¸ƒï¼Œæ”¯æŒæ— ç¼æ¥å…¥[LMDeploy Serving Service](./docs/zh_cn/serving/api_server.md)
- \[2024/01\] æ”¯æŒå¤šæ¨¡å‹ã€å¤šæœºã€å¤šå¡æ¨ç†æœåŠ¡ã€‚ä½¿ç”¨æ–¹æ³•è¯·å‚è€ƒ[æ­¤å¤„](./docs/zh_cn/serving/proxy_server.md)
- \[2024/01\] å¢åŠ  [PyTorch æ¨ç†å¼•æ“](./docs/zh_cn/inference/pytorch.md)ï¼Œä½œä¸º TurboMind å¼•æ“çš„è¡¥å……ã€‚å¸®åŠ©é™ä½å¼€å‘é—¨æ§›ï¼Œå’Œå¿«é€Ÿå®éªŒæ–°ç‰¹æ€§ã€æ–°æŠ€æœ¯

</details>

<details close>
<summary><b>2023</b></summary>

- \[2023/12\] Turbomind æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€‚[Gradio Demo](./examples/vl/README.md)
- \[2023/11\] Turbomind æ”¯æŒç›´æ¥è¯»å– Huggingface æ¨¡å‹ã€‚ç‚¹å‡»[è¿™é‡Œ](docs/zh_cn/inference/load_hf.md)æŸ¥çœ‹ä½¿ç”¨æ–¹æ³•
- \[2023/11\] TurboMind é‡ç£…å‡çº§ã€‚åŒ…æ‹¬ï¼šPaged Attentionã€æ›´å¿«çš„ä¸”ä¸å—åºåˆ—æœ€å¤§é•¿åº¦é™åˆ¶çš„ attention kernelã€2+å€å¿«çš„ KV8 kernelsã€Split-K decoding (Flash Decoding) å’Œ æ”¯æŒ sm_75 æ¶æ„çš„ W4A16
- \[2023/09\] TurboMind æ”¯æŒ Qwen-14B
- \[2023/09\] TurboMind æ”¯æŒ InternLM-20B æ¨¡å‹
- \[2023/09\] TurboMind æ”¯æŒ Code Llama æ‰€æœ‰åŠŸèƒ½ï¼šä»£ç ç»­å†™ã€å¡«ç©ºã€å¯¹è¯ã€Pythonä¸“é¡¹ã€‚ç‚¹å‡»[è¿™é‡Œ](./docs/zh_cn/supported_models/codellama.md)é˜…è¯»éƒ¨ç½²æ–¹æ³•
- \[2023/09\] TurboMind æ”¯æŒ Baichuan2-7B
- \[2023/08\] TurboMind æ”¯æŒ flash-attention2
- \[2023/08\] TurboMind æ”¯æŒ Qwen-7Bï¼ŒåŠ¨æ€NTK-RoPEç¼©æ”¾ï¼ŒåŠ¨æ€logNç¼©æ”¾
- \[2023/08\] TurboMind æ”¯æŒ Windows (tp=1)
- \[2023/08\] TurboMind æ”¯æŒ 4-bit æ¨ç†ï¼Œé€Ÿåº¦æ˜¯ FP16 çš„ 2.4 å€ï¼Œæ˜¯ç›®å‰æœ€å¿«çš„å¼€æºå®ç°ã€‚éƒ¨ç½²æ–¹å¼è¯·çœ‹[è¿™é‡Œ](docs/zh_cn/quantization/w4a16.md)
- \[2023/08\] LMDeploy å¼€é€šäº† [HuggingFace Hub](https://huggingface.co/lmdeploy) ï¼Œæä¾›å¼€ç®±å³ç”¨çš„ 4-bit æ¨¡å‹
- \[2023/08\] LMDeploy æ”¯æŒä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•è¿›è¡Œ 4-bit é‡åŒ–
- \[2023/07\] TurboMind æ”¯æŒä½¿ç”¨ GQA çš„ Llama-2 70B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ Llama-2 7B/13B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ InternLM çš„ Tensor Parallel æ¨ç†

</details>
______________________________________________________________________

# ç®€ä»‹

LMDeploy ç”± [MMDeploy](https://github.com/open-mmlab/mmdeploy) å’Œ [MMRazor](https://github.com/open-mmlab/mmrazor) å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚
è¿™ä¸ªå¼ºå¤§çš„å·¥å…·ç®±æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

- **é«˜æ•ˆçš„æ¨ç†**ï¼šLMDeploy å¼€å‘äº† Persistent Batch(å³ Continuous Batch)ï¼ŒBlocked K/V Cacheï¼ŒåŠ¨æ€æ‹†åˆ†å’Œèåˆï¼Œå¼ é‡å¹¶è¡Œï¼Œé«˜æ•ˆçš„è®¡ç®— kernelç­‰é‡è¦ç‰¹æ€§ã€‚æ¨ç†æ€§èƒ½æ˜¯ vLLM çš„ 1.8 å€

- **å¯é çš„é‡åŒ–**ï¼šLMDeploy æ”¯æŒæƒé‡é‡åŒ–å’Œ k/v é‡åŒ–ã€‚4bit æ¨¡å‹æ¨ç†æ•ˆç‡æ˜¯ FP16 ä¸‹çš„ 2.4 å€ã€‚é‡åŒ–æ¨¡å‹çš„å¯é æ€§å·²é€šè¿‡ OpenCompass è¯„æµ‹å¾—åˆ°å……åˆ†éªŒè¯ã€‚

- **ä¾¿æ·çš„æœåŠ¡**ï¼šé€šè¿‡è¯·æ±‚åˆ†å‘æœåŠ¡ï¼ŒLMDeploy æ”¯æŒå¤šæ¨¡å‹åœ¨å¤šæœºã€å¤šå¡ä¸Šçš„æ¨ç†æœåŠ¡ã€‚

- **æœ‰çŠ¶æ€æ¨ç†**ï¼šé€šè¿‡ç¼“å­˜å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­ attention çš„ k/vï¼Œè®°ä½å¯¹è¯å†å²ï¼Œä»è€Œé¿å…é‡å¤å¤„ç†å†å²ä¼šè¯ã€‚æ˜¾è‘—æå‡é•¿æ–‡æœ¬å¤šè½®å¯¹è¯åœºæ™¯ä¸­çš„æ•ˆç‡ã€‚

# æ€§èƒ½

LMDeploy TurboMind å¼•æ“æ‹¥æœ‰å“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨å„ç§è§„æ¨¡çš„æ¨¡å‹ä¸Šï¼Œæ¯ç§’å¤„ç†çš„è¯·æ±‚æ•°æ˜¯ vLLM çš„ 1.36 ~ 1.85 å€ã€‚åœ¨é™æ€æ¨ç†èƒ½åŠ›æ–¹é¢ï¼ŒTurboMind 4bit æ¨¡å‹æ¨ç†é€Ÿåº¦ï¼ˆout token/sï¼‰è¿œé«˜äº FP16/BF16 æ¨ç†ã€‚åœ¨å° batch æ—¶ï¼Œæé«˜åˆ° 2.4 å€ã€‚

![v0 1 0-benchmark](https://github.com/InternLM/lmdeploy/assets/4560679/8e455cf1-a792-4fa8-91a2-75df96a2a5ba)

æ›´å¤šè®¾å¤‡ã€æ›´å¤šè®¡ç®—ç²¾åº¦ã€æ›´å¤šsettingä¸‹çš„çš„æ¨ç† benchmarkï¼Œè¯·å‚è€ƒä»¥ä¸‹é“¾æ¥ï¼š

- [A100](./docs/en/benchmark/a100_fp16.md)
- 4090
- 3090
- 2080

# æ”¯æŒçš„æ¨¡å‹

|        Model        |    Size     |
| :-----------------: | :---------: |
|        Llama        |  7B - 65B   |
|       Llama2        |  7B - 70B   |
|       Llama3        |   8B, 70B   |
|      InternLM       |  7B - 20B   |
|      InternLM2      |  7B - 20B   |
| InternLM-XComposer  |     7B      |
| InternLM-XComposer2 | 7B, 4khd-7B |
|        QWen         | 1.8B - 72B  |
|       QWen1.5       | 0.5B - 72B  |
|     QWen1.5-MoE     |    A2.7B    |
|       QWen-VL       |     7B      |
|      Baichuan       |     7B      |
|      Baichuan2      |  7B - 13B   |
|     Code Llama      |  7B - 34B   |
|      ChatGLM2       |     6B      |
|       Falcon        |  7B - 180B  |
|         YI          |  6B - 34B   |
|       Mistral       |     7B      |
|    DeepSeek-MoE     |     16B     |
|     DeepSeek-VL     |     7B      |
|    InternVL-Chat    |      -      |
|       Mixtral       |    8x7B     |
|        Gemma        |    2B-7B    |
|        Dbrx         |    132B     |
|   LLaVA(1.5,1.6)    |  7B - 34B   |
|   MiniGeminiLlama   |     7B      |
|     StarCoder2      |   3B-15B    |

LMDeploy æ”¯æŒ 2 ç§æ¨ç†å¼•æ“ï¼š [TurboMind](./docs/zh_cn/inference/turbomind.md) å’Œ [PyTorch](./docs/zh_cn/inference/pytorch.md)ï¼Œå®ƒä»¬ä¾§é‡ä¸åŒã€‚å‰è€…è¿½æ±‚æ¨ç†æ€§èƒ½çš„æè‡´ä¼˜åŒ–ï¼Œåè€…çº¯ç”¨pythonå¼€å‘ï¼Œç€é‡é™ä½å¼€å‘è€…çš„é—¨æ§›ã€‚

å®ƒä»¬åœ¨æ”¯æŒçš„æ¨¡å‹ç±»åˆ«ã€è®¡ç®—ç²¾åº¦æ–¹é¢æœ‰æ‰€å·®åˆ«ã€‚ç”¨æˆ·å¯å‚è€ƒ[è¿™é‡Œ](./docs/zh_cn/supported_models/supported_models.md), æŸ¥é˜…æ¯ä¸ªæ¨ç†å¼•æ“çš„èƒ½åŠ›ï¼Œå¹¶æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„ã€‚

# å¿«é€Ÿå¼€å§‹ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Dh-YlSwg78ZO3AlleO441NF_QP2shs95#scrollTo=YALmXnwCG1pQ)

## å®‰è£…

ä½¿ç”¨ pip ( python 3.8+) å®‰è£… LMDeployï¼Œæˆ–è€…[æºç å®‰è£…](./docs/zh_cn/build.md)

```shell
pip install lmdeploy
```

è‡ª v0.3.0 èµ·ï¼ŒLMDeploy é¢„ç¼–è¯‘åŒ…é»˜è®¤åŸºäº CUDA 12 ç¼–è¯‘ã€‚å¦‚æœéœ€è¦åœ¨ CUDA 11+ ä¸‹å®‰è£… LMDeployï¼Œè¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```shell
export LMDEPLOY_VERSION=0.3.0
export PYTHON_VERSION=38
pip install https://github.com/InternLM/lmdeploy/releases/download/v${LMDEPLOY_VERSION}/lmdeploy-${LMDEPLOY_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux2014_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118
```

## ç¦»çº¿æ‰¹å¤„ç†

```python
import lmdeploy
pipe = lmdeploy.pipeline("internlm/internlm-chat-7b")
response = pipe(["Hi, pls intro yourself", "Shanghai is"])
print(response)
```

> \[!NOTE\]
> LMDeploy é»˜è®¤ä» HuggingFace ä¸Šé¢ä¸‹è½½æ¨¡å‹ï¼Œå¦‚æœè¦ä» ModelScope ä¸Šé¢ä¸‹è½½æ¨¡å‹ï¼Œè¯·é€šè¿‡å‘½ä»¤ `pip install modelscope` å®‰è£…ModelScopeï¼Œå¹¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
>
> `export LMDEPLOY_USE_MODELSCOPE=True`

å…³äº pipeline çš„æ›´å¤šæ¨ç†å‚æ•°è¯´æ˜ï¼Œè¯·å‚è€ƒ[è¿™é‡Œ](./docs/zh_cn/inference/pipeline.md)

# ç”¨æˆ·æ•™ç¨‹

è¯·é˜…è¯»[å¿«é€Ÿä¸Šæ‰‹](./docs/zh_cn/get_started.md)ç« èŠ‚ï¼Œäº†è§£ LMDeploy çš„åŸºæœ¬ç”¨æ³•ã€‚

ä¸ºäº†å¸®åŠ©ç”¨æˆ·æ›´è¿›ä¸€æ­¥äº†è§£ LMDeployï¼Œæˆ‘ä»¬å‡†å¤‡äº†ç”¨æˆ·æŒ‡å—å’Œè¿›é˜¶æŒ‡å—ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„[æ–‡æ¡£](https://lmdeploy.readthedocs.io/zh-cn/latest/)ï¼š

- ç”¨æˆ·æŒ‡å—
  - [LLM æ¨ç† pipeline](./docs/zh_cn/inference/pipeline.md) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Dh-YlSwg78ZO3AlleO441NF_QP2shs95#scrollTo=YALmXnwCG1pQ)
  - [VLM æ¨ç† pipeline](./docs/zh_cn/inference/vl_pipeline.md) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1nKLfnPeDA3p-FMNw2NhI-KOpk7-nlNjF?usp=sharing)
  - [LLM æ¨ç†æœåŠ¡](./docs/zh_cn/serving/api_server.md)
  - [VLM æ¨ç†æœåŠ¡](./docs/zh_cn/serving/api_server_vl.md)
  - [æ¨¡å‹é‡åŒ–](./docs/zh_cn/quantization)
- è¿›é˜¶æŒ‡å—
  - [æ¨ç†å¼•æ“ - TurboMind](./docs/zh_cn/inference/turbomind.md)
  - [æ¨ç†å¼•æ“ - PyTorch](./docs/zh_cn/inference/pytorch.md)
  - [è‡ªå®šä¹‰å¯¹è¯æ¨¡æ¿](./docs/zh_cn/advance/chat_template.md)
  - [æ”¯æŒæ–°æ¨¡å‹](./docs/zh_cn/advance/pytorch_new_model.md)
  - gemm tuning
  - [é•¿æ–‡æœ¬æ¨ç†](./docs/zh_cn/advance/long_context.md)
  - [å¤šæ¨¡å‹æ¨ç†æœåŠ¡](./docs/zh_cn/serving/proxy_server.md)

# ç¤¾åŒºé¡¹ç›®

- ä½¿ç”¨LMDeployåœ¨è‹±ä¼Ÿè¾¾Jetsonç³»åˆ—æ¿å¡éƒ¨ç½²å¤§æ¨¡å‹ï¼š[LMDeploy-Jetson](https://github.com/BestAnHongjun/LMDeploy-Jetson)

# è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ LMDeploy æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚è¯·å‚è€ƒ[è´¡çŒ®æŒ‡å—](.github/CONTRIBUTING.md)æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

# è‡´è°¢

- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [llm-awq](https://github.com/mit-han-lab/llm-awq)
- [vLLM](https://github.com/vllm-project/vllm)
- [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII)

# å¼•ç”¨

```bibtex
@misc{2023lmdeploy,
    title={LMDeploy: A Toolkit for Compressing, Deploying, and Serving LLM},
    author={LMDeploy Contributors},
    howpublished = {\url{https://github.com/InternLM/lmdeploy}},
    year={2023}
}
```

# å¼€æºè®¸å¯è¯

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache 2.0 å¼€æºè®¸å¯è¯](LICENSE)ã€‚
