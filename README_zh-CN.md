<div align="center">
  <img src="resources/lmdeploy-logo.svg" width="450"/>

[![docs](https://img.shields.io/badge/docs-latest-blue)](https://lmdeploy-zh-cn.readthedocs.io/zh_CN/latest/)
[![badge](https://github.com/InternLM/lmdeploy/workflows/lint/badge.svg)](https://github.com/InternLM/lmdeploy/actions)
[![PyPI](https://img.shields.io/pypi/v/lmdeploy)](https://pypi.org/project/lmdeploy)
[![license](https://img.shields.io/github/license/InternLM/lmdeploy.svg)](https://github.com/InternLM/lmdeploy/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)
[![open issues](https://img.shields.io/github/issues-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)

[English](README.md) | ç®€ä½“ä¸­æ–‡

</div>

<p align="center">
    ğŸ‘‹ join us on <a href="https://twitter.com/intern_lm" target="_blank">Twitter</a>, <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> and <a href="https://r.vansin.top/?r=internwx" target="_blank">WeChat</a>
</p>

______________________________________________________________________

## æ›´æ–° ğŸ‰

- \[2023/11\] Turbomind æ”¯æŒç›´æ¥è¯»å– Huggingface æ¨¡å‹ã€‚ç‚¹å‡»[è¿™é‡Œ](./docs/en/load_hf.md)æŸ¥çœ‹ä½¿ç”¨æ–¹æ³•
- \[2023/11\] TurboMind é‡ç£…å‡çº§ã€‚åŒ…æ‹¬ï¼šPaged Attentionã€æ›´å¿«çš„ä¸”ä¸å—åºåˆ—æœ€å¤§é•¿åº¦é™åˆ¶çš„ attention kernelã€2+å€å¿«çš„ KV8 kernelsã€Split-K decoding (Flash Decoding) å’Œ æ”¯æŒ sm_75 æ¶æ„çš„ W4A16
- \[2023/09\] TurboMind æ”¯æŒ Qwen-14B
- \[2023/09\] TurboMind æ”¯æŒ InternLM-20B æ¨¡å‹
- \[2023/09\] TurboMind æ”¯æŒ Code Llama æ‰€æœ‰åŠŸèƒ½ï¼šä»£ç ç»­å†™ã€å¡«ç©ºã€å¯¹è¯ã€Pythonä¸“é¡¹ã€‚ç‚¹å‡»[è¿™é‡Œ](./docs/zh_cn/supported_models/codellama.md)é˜…è¯»éƒ¨ç½²æ–¹æ³•
- \[2023/09\] TurboMind æ”¯æŒ Baichuan2-7B
- \[2023/08\] TurboMind æ”¯æŒ flash-attention2
- \[2023/08\] TurboMind æ”¯æŒ Qwen-7Bï¼ŒåŠ¨æ€NTK-RoPEç¼©æ”¾ï¼ŒåŠ¨æ€logNç¼©æ”¾
- \[2023/08\] TurboMind æ”¯æŒ Windows (tp=1)
- \[2023/08\] TurboMind æ”¯æŒ 4-bit æ¨ç†ï¼Œé€Ÿåº¦æ˜¯ FP16 çš„ 2.4 å€ï¼Œæ˜¯ç›®å‰æœ€å¿«çš„å¼€æºå®ç°ğŸš€ã€‚éƒ¨ç½²æ–¹å¼è¯·çœ‹[è¿™é‡Œ](./docs/zh_cn/w4a16.md)
- \[2023/08\] LMDeploy å¼€é€šäº† [HuggingFace Hub](https://huggingface.co/lmdeploy) ï¼Œæä¾›å¼€ç®±å³ç”¨çš„ 4-bit æ¨¡å‹
- \[2023/08\] LMDeploy æ”¯æŒä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•è¿›è¡Œ 4-bit é‡åŒ–
- \[2023/07\] TurboMind æ”¯æŒä½¿ç”¨ GQA çš„ Llama-2 70B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ Llama-2 7B/13B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ InternLM çš„ Tensor Parallel æ¨ç†

______________________________________________________________________

## ç®€ä»‹

LMDeploy ç”± [MMDeploy](https://github.com/open-mmlab/mmdeploy) å’Œ [MMRazor](https://github.com/open-mmlab/mmrazor) å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚
è¿™ä¸ªå¼ºå¤§çš„å·¥å…·ç®±æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

- **é«˜æ•ˆæ¨ç†å¼•æ“ TurboMind**ï¼šåŸºäº [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)ï¼Œæˆ‘ä»¬å®ç°äº†é«˜æ•ˆæ¨ç†å¼•æ“ TurboMindï¼Œæ”¯æŒ InternLMã€LLaMAã€vicunaç­‰æ¨¡å‹åœ¨ NVIDIA GPU ä¸Šçš„æ¨ç†ã€‚

- **äº¤äº’æ¨ç†æ–¹å¼**ï¼šé€šè¿‡ç¼“å­˜å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­ attention çš„ k/vï¼Œè®°ä½å¯¹è¯å†å²ï¼Œä»è€Œé¿å…é‡å¤å¤„ç†å†å²ä¼šè¯ã€‚

- **å¤š GPU éƒ¨ç½²å’Œé‡åŒ–**ï¼šæˆ‘ä»¬æä¾›äº†å…¨é¢çš„æ¨¡å‹éƒ¨ç½²å’Œé‡åŒ–æ”¯æŒï¼Œå·²åœ¨ä¸åŒè§„æ¨¡ä¸Šå®ŒæˆéªŒè¯ã€‚

- **persistent batch æ¨ç†**ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ‰§è¡Œæ•ˆç‡ã€‚

  ![PersistentBatchInference](https://github.com/InternLM/lmdeploy/assets/67539920/e3876167-0671-44fc-ac52-5a0f9382493e)

## æ”¯æŒçš„æ¨¡å‹

`LMDeploy` æ”¯æŒ `TurboMind` å’Œ `Pytorch` ä¸¤ç§æ¨ç†åç«¯ã€‚è¿è¡Œ`lmdeploy list`å¯æŸ¥çœ‹æ”¯æŒæ¨¡å‹åˆ—è¡¨

### TurboMind

> **Note**<br />
> W4A16 æ¨ç†éœ€è¦ Ampere åŠä»¥ä¸Šæ¶æ„çš„ Nvidia GPU

|     æ¨¡å‹     | æ¨¡å‹å¹¶è¡Œ | FP16 | KV INT8 | W4A16 | W8A8 |
| :----------: | :------: | :--: | :-----: | :---: | :--: |
|    Llama     |   Yes    | Yes  |   Yes   |  Yes  |  No  |
|    Llama2    |   Yes    | Yes  |   Yes   |  Yes  |  No  |
|    SOLAR     |   Yes    | Yes  |   Yes   |  Yes  |  No  |
| InternLM-7B  |   Yes    | Yes  |   Yes   |  Yes  |  No  |
| InternLM-20B |   Yes    | Yes  |   Yes   |  Yes  |  No  |
|   QWen-7B    |   Yes    | Yes  |   Yes   |  Yes  |  No  |
|   QWen-14B   |   Yes    | Yes  |   Yes   |  Yes  |  No  |
| Baichuan-7B  |   Yes    | Yes  |   Yes   |  Yes  |  No  |
| Baichuan2-7B |   Yes    | Yes  |   Yes   |  Yes  |  No  |
|  Code Llama  |   Yes    | Yes  |   No    |  No   |  No  |

### Pytorch

|    æ¨¡å‹     | æ¨¡å‹å¹¶è¡Œ | FP16 | KV INT8 | W4A16 | W8A8 |
| :---------: | :------: | :--: | :-----: | :---: | :--: |
|    Llama    |   Yes    | Yes  |   No    |  No   |  No  |
|   Llama2    |   Yes    | Yes  |   No    |  No   |  No  |
| InternLM-7B |   Yes    | Yes  |   No    |  No   |  No  |

## æ€§èƒ½

**åœºæ™¯ä¸€**: å›ºå®šçš„è¾“å…¥ã€è¾“å‡ºtokenæ•°ï¼ˆ1,2048ï¼‰ï¼Œæµ‹è¯• output token throughput

**åœºæ™¯äºŒ**: ä½¿ç”¨çœŸå®æ•°æ®ï¼Œæµ‹è¯• request throughput

æµ‹è¯•é…ç½®ï¼šLLaMA-7B, NVIDIA A100(80G)

TurboMind çš„ output token throughput è¶…è¿‡ 2000 token/s, æ•´ä½“æ¯” DeepSpeed æå‡çº¦ 5% - 15%ï¼Œæ¯” huggingface transformers æå‡ 2.3 å€
åœ¨ request throughput æŒ‡æ ‡ä¸Šï¼ŒTurboMind çš„æ•ˆç‡æ¯” vLLM é«˜ 30%

![benchmark](https://github.com/InternLM/lmdeploy/assets/4560679/7775c518-608e-4e5b-be73-7645a444e774)

## å¿«é€Ÿä¸Šæ‰‹

### å®‰è£…

ä½¿ç”¨ pip ( python 3.8+) å®‰è£… LMDeployï¼Œæˆ–è€…[æºç å®‰è£…](./docs/zh_cn/build.md)

```shell
pip install lmdeploy
```

> **Note**<br />
> `pip install lmdeploy`é»˜è®¤å®‰è£…runtimeä¾èµ–åŒ…ï¼Œä½¿ç”¨lmdeployçš„liteå’ŒserveåŠŸèƒ½æ—¶ï¼Œç”¨æˆ·éœ€è¦å®‰è£…é¢å¤–ä¾èµ–åŒ…ã€‚ä¾‹å¦‚: `pip install lmdeploy[lite]` ä¼šé¢å¤–å®‰è£…`lmdeploy.lite`æ¨¡å—çš„ä¾èµ–åŒ…
>
> - `all`: å®‰è£…`lmdeploy`æ‰€æœ‰ä¾èµ–åŒ…ï¼Œå…·ä½“å¯æŸ¥çœ‹`requirements.txt`
> - `lite`: é¢å¤–å®‰è£…`lmdeploy.lite`æ¨¡å—çš„ä¾èµ–åŒ…ï¼Œå…·ä½“å¯æŸ¥çœ‹`requirements/lite.txt`
> - `serve`: é¢å¤–å®‰è£…`lmdeploy.serve`æ¨¡å—çš„ä¾èµ–åŒ…ï¼Œå…·ä½“å¯æŸ¥çœ‹`requirements/serve.txt`

### éƒ¨ç½² InternLM

ä½¿ç”¨ TurboMind æ¨ç†æ¨¡å‹éœ€è¦å…ˆå°†æ¨¡å‹è½¬åŒ–ä¸º TurboMind çš„æ ¼å¼ï¼Œç›®å‰æ”¯æŒåœ¨çº¿è½¬æ¢å’Œç¦»çº¿è½¬æ¢ä¸¤ç§å½¢å¼ã€‚åœ¨çº¿è½¬æ¢å¯ä»¥ç›´æ¥åŠ è½½ Huggingface æ¨¡å‹ï¼Œç¦»çº¿è½¬æ¢éœ€éœ€è¦å…ˆä¿å­˜æ¨¡å‹å†åŠ è½½ã€‚

ä¸‹é¢ä»¥ [internlm/internlm-chat-7b-v1_1](https://huggingface.co/internlm/internlm-chat-7b-v1_1) ä¸ºä¾‹ï¼Œå±•ç¤ºåœ¨çº¿è½¬æ¢çš„ä½¿ç”¨æ–¹å¼ã€‚å…¶ä»–æ–¹å¼å¯å‚è€ƒ[load_hf.md](docs/zh_cn/load_hf.md)

#### ä½¿ç”¨ turbomind æ¨ç†

```shell
lmdeploy chat turbomind internlm/internlm-chat-7b-v1_1 --model-name internlm-chat-7b
```

> **Note**<br /> internlm/internlm-chat-7b-v1_1 ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° `.cache` æ–‡ä»¶å¤¹ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥ä¼ ä¸‹è½½å¥½çš„è·¯å¾„ã€‚

> **Note**<br />
> turbomind åœ¨ä½¿ç”¨ FP16 ç²¾åº¦æ¨ç† InternLM-7B æ¨¡å‹æ—¶ï¼Œæ˜¾å­˜å¼€é”€è‡³å°‘éœ€è¦ 15.7Gã€‚å»ºè®®ä½¿ç”¨ 3090, V100ï¼ŒA100ç­‰å‹å·çš„æ˜¾å¡ã€‚<br />
> å…³é—­æ˜¾å¡çš„ ECC å¯ä»¥è…¾å‡º 10% æ˜¾å­˜ï¼Œæ‰§è¡Œ `sudo nvidia-smi --ecc-config=0` é‡å¯ç³»ç»Ÿç”Ÿæ•ˆã€‚

> **Note**<br />
> ä½¿ç”¨ Tensor å¹¶å‘å¯ä»¥åˆ©ç”¨å¤šå¼  GPU è¿›è¡Œæ¨ç†ã€‚åœ¨ `chat` æ—¶æ·»åŠ å‚æ•° `--tp=<num_gpu>` å¯ä»¥å¯åŠ¨è¿è¡Œæ—¶ TPã€‚

#### å¯åŠ¨ gradio server

```shell
# å®‰è£…lmdeployé¢å¤–ä¾èµ–
pip install lmdeploy[serve]

lmdeploy serve gradio internlm/internlm-chat-7b-v1_1 --model-name internlm-chat-7b
```

![](https://github.com/InternLM/lmdeploy/assets/67539920/08d1e6f2-3767-44d5-8654-c85767cec2ab)

#### é€šè¿‡ Restful API éƒ¨ç½²æœåŠ¡

ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š

```shell
# å®‰è£…lmdeployé¢å¤–ä¾èµ–
pip install lmdeploy[serve]

lmdeploy serve api_server internlm/internlm-chat-7b-v1_1 --model-name internlm-chat-7b --instance_num 32 --tp 1
```

ä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨ç†æœåŠ¡è¿›è¡Œå¯¹è¯ï¼š

```shell
# api_server_url is what printed in api_server.py, e.g. http://localhost:23333
lmdeploy serve api_client api_server_url
```

ä¹Ÿå¯ä»¥é€šè¿‡ WebUI æ–¹å¼æ¥å¯¹è¯ï¼š

```shell
# api_server_url is what printed in api_server.py, e.g. http://localhost:23333
# server_ip and server_port here are for gradio ui
# example: lmdeploy serve gradio http://localhost:23333 --server_name localhost --server_port 6006
lmdeploy serve gradio api_server_url --server_name ${gradio_ui_ip} --server_port ${gradio_ui_port}
```

æ›´å¤šè¯¦æƒ…å¯ä»¥æŸ¥é˜… [restful_api.md](docs/zh_cn/restful_api.md)ã€‚

### åŸºäº PyTorch çš„æ¨ç†

ä½ å¿…é¡»ç¡®ä¿ç¯å¢ƒä¸­æœ‰å®‰è£… deepspeedï¼š

```
pip install deepspeed
```

#### å•ä¸ª GPU

```shell
lmdeploy chat torch $NAME_OR_PATH_TO_HF_MODEL\
    --max_new_tokens 64 \
    --temperture 0.8 \
    --top_p 0.95 \
    --seed 0
```

#### ä½¿ç”¨ DeepSpeed å®ç°å¼ é‡å¹¶è¡Œ

```shell
deepspeed --module --num_gpus 2 lmdeploy.pytorch.chat \
    $NAME_OR_PATH_TO_HF_MODEL \
    --max_new_tokens 64 \
    --temperture 0.8 \
    --top_p 0.95 \
    --seed 0
```

## é‡åŒ–éƒ¨ç½²

#### æƒé‡ INT4 é‡åŒ–

LMDeploy ä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•å¯¹æ¨¡å‹æƒé‡è¿›è¡Œé‡åŒ–

[ç‚¹å‡»è¿™é‡Œ](./docs/zh_cn/w4a16.md) æŸ¥çœ‹ weight int4 ç”¨æ³•æµ‹è¯•ç»“æœã€‚

#### KV Cache INT8 é‡åŒ–

[ç‚¹å‡»è¿™é‡Œ](./docs/zh_cn/kv_int8.md) æŸ¥çœ‹ kv int8 ä½¿ç”¨æ–¹æ³•ã€å®ç°å…¬å¼å’Œæµ‹è¯•ç»“æœã€‚

> **Warning**<br />
> é‡åŒ–éƒ¨ç½²ä¸æ”¯æŒè¿è¡Œæ—¶ Tensor å¹¶å‘ã€‚å¦‚æœå¸Œæœ›ä½¿ç”¨ Tensor å¹¶å‘ï¼Œéœ€è¦åœ¨ deploy æ—¶é…ç½® tp å‚æ•°ã€‚

## è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ LMDeploy æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚è¯·å‚è€ƒ[è´¡çŒ®æŒ‡å—](.github/CONTRIBUTING.md)æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

## è‡´è°¢

- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [llm-awq](https://github.com/mit-han-lab/llm-awq)

## License

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache 2.0 å¼€æºè®¸å¯è¯](LICENSE)ã€‚
