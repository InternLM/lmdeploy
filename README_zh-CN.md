<div align="center">
  <img src="resources/lmdeploy-logo.png" width="450"/>

[![docs](https://img.shields.io/badge/docs-latest-blue)](https://lmdeploy-zh-cn.readthedocs.io/zh_CN/latest/)
[![badge](https://github.com/InternLM/lmdeploy/workflows/lint/badge.svg)](https://github.com/InternLM/lmdeploy/actions)
[![codecov](https://codecov.io/gh/InternLM/lmdeploy/branch/main/graph/badge.svg)](https://codecov.io/gh/InternLM/lmdeploy)
[![license](https://img.shields.io/github/license/InternLM/lmdeploy.svg)](https://github.com/InternLM/lmdeploy/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)
[![open issues](https://img.shields.io/github/issues-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)

[English](README.md) | ç®€ä½“ä¸­æ–‡

</div>

<p align="center">
    ğŸ‘‹ join us on <a href="https://twitter.com/intern_lm" target="_blank">Twitter</a>, <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> and <a href="https://r.vansin.top/?r=internwx" target="_blank">WeChat</a>
</p>

______________________________________________________________________

## æ›´æ–° ğŸ‰

- \[2023/08\] TurboMind æ”¯æŒ 4-bit æ¨ç†ï¼Œé€Ÿåº¦æ˜¯ FP16 çš„ 2.4 å€ï¼Œæ˜¯ç›®å‰æœ€å¿«çš„å¼€æºå®ç°ğŸš€ã€‚éƒ¨ç½²æ–¹å¼è¯·çœ‹[è¿™é‡Œ](./docs/zh_cn/w4a16.md)
- \[2023/08\] LMDeploy å¼€é€šäº† [HuggingFace Hub](https://huggingface.co/lmdeploy) ï¼Œæä¾›å¼€ç®±å³ç”¨çš„ 4-bit æ¨¡å‹
- \[2023/08\] LMDeploy æ”¯æŒä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•è¿›è¡Œ 4-bit é‡åŒ–
- \[2023/07\] TurboMind æ”¯æŒä½¿ç”¨ GQA çš„ Llama-2 70B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ Llama-2 7B/13B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ InternLM çš„ Tensor Parallel æ¨ç†

______________________________________________________________________

## ç®€ä»‹

LMDeploy ç”± [MMDeploy](https://github.com/open-mmlab/mmdeploy) å’Œ [MMRazor](https://github.com/open-mmlab/mmrazor) å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚
è¿™ä¸ªå¼ºå¤§çš„å·¥å…·ç®±æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

- **é«˜æ•ˆæ¨ç†å¼•æ“ TurboMind**ï¼šåŸºäº [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)ï¼Œæˆ‘ä»¬å®ç°äº†é«˜æ•ˆæ¨ç†å¼•æ“ TurboMindï¼Œæ”¯æŒ InternLMã€LLaMAã€vicunaç­‰æ¨¡å‹åœ¨ NVIDIA GPU ä¸Šçš„æ¨ç†ã€‚

- **äº¤äº’æ¨ç†æ–¹å¼**ï¼šé€šè¿‡ç¼“å­˜å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­ attention çš„ k/vï¼Œè®°ä½å¯¹è¯å†å²ï¼Œä»è€Œé¿å…é‡å¤å¤„ç†å†å²ä¼šè¯ã€‚

- **å¤š GPU éƒ¨ç½²å’Œé‡åŒ–**ï¼šæˆ‘ä»¬æä¾›äº†å…¨é¢çš„æ¨¡å‹éƒ¨ç½²å’Œé‡åŒ–æ”¯æŒï¼Œå·²åœ¨ä¸åŒè§„æ¨¡ä¸Šå®ŒæˆéªŒè¯ã€‚

- **persistent batch æ¨ç†**ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ‰§è¡Œæ•ˆç‡ã€‚

  ![PersistentBatchInference](https://github.com/InternLM/lmdeploy/assets/67539920/e3876167-0671-44fc-ac52-5a0f9382493e)

## æ”¯æŒçš„æ¨¡å‹

`LMDeploy` æ”¯æŒ `TurboMind` å’Œ `Pytorch` ä¸¤ç§æ¨ç†åç«¯

### TurboMind

> **Note**<br />
> W4A16 æ¨ç†éœ€è¦ Ampere åŠä»¥ä¸Šæ¶æ„çš„ Nvidia GPU

|   æ¨¡å‹   | æ¨¡å‹å¹¶è¡Œ | FP16 | KV INT8 | W4A16 | W8A8 |
| :------: | :------: | :--: | :-----: | :---: | :--: |
|  Llama   |   Yes    | Yes  |   Yes   |  Yes  |  No  |
|  Llama2  |   Yes    | Yes  |   Yes   |  Yes  |  No  |
| InternLM |   Yes    | Yes  |   Yes   |  Yes  |  No  |

### Pytorch

|   æ¨¡å‹   | æ¨¡å‹å¹¶è¡Œ | FP16 | KV INT8 | W4A16 | W8A8 |
| :------: | :------: | :--: | :-----: | :---: | :--: |
|  Llama   |   Yes    | Yes  |   No    |  No   |  No  |
|  Llama2  |   Yes    | Yes  |   No    |  No   |  No  |
| InternLM |   Yes    | Yes  |   No    |  No   |  No  |

## æ€§èƒ½

**åœºæ™¯ä¸€**: å›ºå®šçš„è¾“å…¥ã€è¾“å‡ºtokenæ•°ï¼ˆ1,2048ï¼‰ï¼Œæµ‹è¯• output token throughput

**åœºæ™¯äºŒ**: ä½¿ç”¨çœŸå®æ•°æ®ï¼Œæµ‹è¯• request throughput

æµ‹è¯•é…ç½®ï¼šLLaMA-7B, NVIDIA A100(80G)

TurboMind çš„ output token throughput è¶…è¿‡ 2000 token/s, æ•´ä½“æ¯” DeepSpeed æå‡çº¦ 5% - 15%ï¼Œæ¯” huggingface transformers æå‡ 2.3 å€
åœ¨ request throughput æŒ‡æ ‡ä¸Šï¼ŒTurboMind çš„æ•ˆç‡æ¯” vLLM é«˜ 30%

![benchmark](https://github.com/InternLM/lmdeploy/assets/4560679/7775c518-608e-4e5b-be73-7645a444e774)

## å¿«é€Ÿä¸Šæ‰‹

### å®‰è£…

ä½¿ç”¨ pip ( python 3.8+) å®‰è£… LMDeployï¼Œæˆ–è€…[æºç å®‰è£…](./docs/zh_cn/build.md)

```shell
pip install lmdeploy
```

### éƒ¨ç½² InternLM

#### è·å– InternLM æ¨¡å‹

```shell
# 1. ä¸‹è½½ InternLM æ¨¡å‹

# Make sure you have git-lfs installed (https://git-lfs.com)
git lfs install
git clone https://huggingface.co/internlm/internlm-chat-7b /path/to/internlm-chat-7b

# if you want to clone without large files â€“ just their pointers
# prepend your git clone with the following env var:
GIT_LFS_SKIP_SMUDGE=1

# 2. è½¬æ¢ä¸º trubomind è¦æ±‚çš„æ ¼å¼ã€‚é»˜è®¤å­˜æ”¾è·¯å¾„ä¸º ./workspace
python3 -m lmdeploy.serve.turbomind.deploy internlm-chat-7b /path/to/internlm-chat-7b

```

#### ä½¿ç”¨ turbomind æ¨ç†

```shell
python3 -m lmdeploy.turbomind.chat ./workspace
```

> **Note**<br />
> turbomind åœ¨ä½¿ç”¨ FP16 ç²¾åº¦æ¨ç† InternLM-7B æ¨¡å‹æ—¶ï¼Œæ˜¾å­˜å¼€é”€è‡³å°‘éœ€è¦ 15.7Gã€‚å»ºè®®ä½¿ç”¨ 3090, V100ï¼ŒA100ç­‰å‹å·çš„æ˜¾å¡ã€‚<br />
> å…³é—­æ˜¾å¡çš„ ECC å¯ä»¥è…¾å‡º 10% æ˜¾å­˜ï¼Œæ‰§è¡Œ `sudo nvidia-smi --ecc-config=0` é‡å¯ç³»ç»Ÿç”Ÿæ•ˆã€‚

> **Note**<br />
> ä½¿ç”¨ Tensor å¹¶å‘å¯ä»¥åˆ©ç”¨å¤šå¼  GPU è¿›è¡Œæ¨ç†ã€‚åœ¨ `chat` æ—¶æ·»åŠ å‚æ•° `--tp=<num_gpu>` å¯ä»¥å¯åŠ¨è¿è¡Œæ—¶ TPã€‚

#### å¯åŠ¨ gradio server

```shell
python3 -m lmdeploy.serve.gradio.app ./workspace
```

![](https://github.com/InternLM/lmdeploy/assets/67539920/08d1e6f2-3767-44d5-8654-c85767cec2ab)

#### é€šè¿‡å®¹å™¨éƒ¨ç½²æ¨ç†æœåŠ¡

ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š

```shell
bash workspace/service_docker_up.sh
```

ä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œæ–¹å¼ä¸æ¨ç†æœåŠ¡è¿›è¡Œå¯¹è¯ï¼š

```shell
python3 -m lmdeploy.serve.client {server_ip_addresss}:33337
```

ä¹Ÿå¯ä»¥é€šè¿‡ WebUI æ–¹å¼æ¥å¯¹è¯ï¼š

```shell
python3 -m lmdeploy.serve.gradio.app {server_ip_addresss}:33337
```

å…¶ä»–æ¨¡å‹çš„éƒ¨ç½²æ–¹å¼ï¼Œæ¯”å¦‚ LLaMAï¼ŒLLaMA-2ï¼Œvicunaç­‰ç­‰ï¼Œè¯·å‚è€ƒ[è¿™é‡Œ](docs/zh_cn/serving.md)

### åŸºäº PyTorch çš„æ¨ç†

ä½ å¿…é¡»ç¡®ä¿ç¯å¢ƒä¸­æœ‰å®‰è£… deepspeedï¼š

```
pip install deepspeed
```

#### å•ä¸ª GPU

```shell
python3 -m lmdeploy.pytorch.chat $NAME_OR_PATH_TO_HF_MODEL\
    --max_new_tokens 64 \
    --temperture 0.8 \
    --top_p 0.95 \
    --seed 0
```

#### ä½¿ç”¨ DeepSpeed å®ç°å¼ é‡å¹¶è¡Œ

```shell
deepspeed --module --num_gpus 2 lmdeploy.pytorch.chat \
    $NAME_OR_PATH_TO_HF_MODEL \
    --max_new_tokens 64 \
    --temperture 0.8 \
    --top_p 0.95 \
    --seed 0
```

## é‡åŒ–éƒ¨ç½²

### Step 1. è·å–é‡åŒ–å‚æ•°

é¦–å…ˆï¼Œæ‰§è¡Œé‡åŒ–è„šæœ¬ï¼Œè·å–é‡åŒ–å‚æ•°

> æ‰§è¡Œåï¼Œé‡åŒ–éœ€è¦çš„å„ç§å‚æ•°ä¼šå­˜æ”¾åœ¨ $WORK_DIR ä¸­; æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­ä¼šç”¨åˆ°

```

python3 -m lmdeploy.lite.apis.calibrate \
  --model $HF_MODEL \
  --calib_dataset 'c4' \             # æ ¡å‡†æ•°æ®é›†ï¼Œæ”¯æŒ c4, ptb, wikitext2, pileval
  --calib_samples 128 \              # æ ¡å‡†é›†çš„æ ·æœ¬æ•°ï¼Œå¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå¯ä»¥é€‚å½“è°ƒå°
  --calib_seqlen 2048 \              # å•æ¡çš„æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå¯ä»¥é€‚å½“è°ƒå°
  --work_dir $WORK_DIR \             # ä¿å­˜ Pytorch æ ¼å¼é‡åŒ–ç»Ÿè®¡å‚æ•°å’Œé‡åŒ–åæƒé‡çš„æ–‡ä»¶å¤¹
```

### Step 2. å®é™…é‡åŒ–æ¨¡å‹

ç›®å‰æ”¯æŒå¯¹æƒé‡çš„ INT4 é‡åŒ–å’Œ KV Cache çš„ INT8 é‡åŒ–ï¼Œæ ¹æ®éœ€æ±‚æ‰§è¡Œå¯¹åº”è„šæœ¬å³å¯

#### æƒé‡ INT4 é‡åŒ–

LMDeploy ä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•å¯¹æ¨¡å‹æƒé‡è¿›è¡Œé‡åŒ–

> éœ€è¦è¾“å…¥ç¬¬ä¸€æ­¥çš„ \`$WORK_DIR\`\` ï¼Œé‡åŒ–åçš„æƒé‡ä¹Ÿä¼šå­˜åœ¨è¿™ä¸ªæ–‡ä»¶å¤¹ä¸­

```
python3 -m lmdeploy.lite.apis.auto_awq \
  --model $HF_MODEL \
  --w_bits 4 \                       # æƒé‡é‡åŒ–çš„ bit æ•°
  --w_group_size 128 \               # æƒé‡é‡åŒ–åˆ†ç»„ç»Ÿè®¡å°ºå¯¸
  --work_dir $WORK_DIR \             # Step 1 ä¿å­˜é‡åŒ–å‚æ•°çš„ç›®å½•
```

#### KV Cache INT8 é‡åŒ–

é¦–å…ˆï¼Œå¯¼å‡º TurboMind æ ¼å¼çš„é‡åŒ–å‚æ•°ï¼ˆKV Cache INT8 é‡åŒ–éœ€è¦ä½¿ç”¨ `TurboMind`ï¼‰

> `$TURBOMIND_DIR` ä¸º  `deploy.py` è½¬æ¢å¾—åˆ°çš„`workspace/triton_models/weights\` ç›®å½•

```
python3 -m lmdeploy.lite.apis.kv_qparams \
  --work_dir $WORK_DIR \              # Step 1 ä¿å­˜é‡åŒ–å‚æ•°çš„ç›®å½•
  --turbomind_dir $TURBOMIND_DIR \
  --kv_sym False \                    # å¯¹ç§°é‡åŒ–æˆ–éå¯¹ç§°é‡åŒ–ï¼Œé»˜è®¤ä¸º False
  --num_tp 1  \                       # Tensor å¹¶è¡Œä½¿ç”¨çš„ GPU æ•°ï¼Œå’Œ deploy.py ä¿æŒä¸€è‡´
```

ç„¶åè°ƒæ•´ `workspace/triton_models/weights/config.ini`

- `use_context_fmha` æ”¹ä¸º 0ï¼Œè¡¨ç¤ºå…³é—­
- `quant_policy` è®¾ç½®ä¸º 4ã€‚æ­¤å‚æ•°é»˜è®¤ä¸º 0ï¼Œè¡¨ç¤ºä¸å¼€å¯

è¿™é‡Œæ˜¯[é‡åŒ–æµ‹è¯•ç»“æœ](./docs/zh_cn/quantization.md)ã€‚

> **Warning**<br />
> é‡åŒ–éƒ¨ç½²ä¸æ”¯æŒè¿è¡Œæ—¶ Tensor å¹¶å‘ã€‚å¦‚æœå¸Œæœ›ä½¿ç”¨ Tensor å¹¶å‘ï¼Œéœ€è¦åœ¨ deploy æ—¶é…ç½® tp å‚æ•°ã€‚

## è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ LMDeploy æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚è¯·å‚è€ƒ[è´¡çŒ®æŒ‡å—](.github/CONTRIBUTING.md)æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

## è‡´è°¢

- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [llm-awq](https://github.com/mit-han-lab/llm-awq)

## License

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache 2.0 å¼€æºè®¸å¯è¯](LICENSE)ã€‚
