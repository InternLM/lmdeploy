<div align="center">
  <img src="resources/lmdeploy-logo.svg" width="450"/>

[![docs](https://img.shields.io/badge/docs-latest-blue)](https://lmdeploy-zh-cn.readthedocs.io/zh_CN/latest/)
[![badge](https://github.com/InternLM/lmdeploy/workflows/lint/badge.svg)](https://github.com/InternLM/lmdeploy/actions)
[![PyPI](https://img.shields.io/pypi/v/lmdeploy)](https://pypi.org/project/lmdeploy)
[![license](https://img.shields.io/github/license/InternLM/lmdeploy.svg)](https://github.com/InternLM/lmdeploy/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)
[![open issues](https://img.shields.io/github/issues-raw/InternLM/lmdeploy)](https://github.com/InternLM/lmdeploy/issues)

[English](README.md) | ç®€ä½“ä¸­æ–‡

</div>

<p align="center">
    ğŸ‘‹ join us on <a href="https://twitter.com/intern_lm" target="_blank">Twitter</a>, <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> and <a href="https://r.vansin.top/?r=internwx" target="_blank">WeChat</a>
</p>

______________________________________________________________________

## æœ€æ–°è¿›å±• ğŸ‰

- \[2023/12\] Turbomind æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€‚[Gradio Demo](./examples/vl/README.md)
- \[2023/11\] Turbomind æ”¯æŒç›´æ¥è¯»å– Huggingface æ¨¡å‹ã€‚ç‚¹å‡»[è¿™é‡Œ](./docs/en/load_hf.md)æŸ¥çœ‹ä½¿ç”¨æ–¹æ³•
- \[2023/11\] TurboMind é‡ç£…å‡çº§ã€‚åŒ…æ‹¬ï¼šPaged Attentionã€æ›´å¿«çš„ä¸”ä¸å—åºåˆ—æœ€å¤§é•¿åº¦é™åˆ¶çš„ attention kernelã€2+å€å¿«çš„ KV8 kernelsã€Split-K decoding (Flash Decoding) å’Œ æ”¯æŒ sm_75 æ¶æ„çš„ W4A16
- \[2023/09\] TurboMind æ”¯æŒ Qwen-14B
- \[2023/09\] TurboMind æ”¯æŒ InternLM-20B æ¨¡å‹
- \[2023/09\] TurboMind æ”¯æŒ Code Llama æ‰€æœ‰åŠŸèƒ½ï¼šä»£ç ç»­å†™ã€å¡«ç©ºã€å¯¹è¯ã€Pythonä¸“é¡¹ã€‚ç‚¹å‡»[è¿™é‡Œ](./docs/zh_cn/supported_models/codellama.md)é˜…è¯»éƒ¨ç½²æ–¹æ³•
- \[2023/09\] TurboMind æ”¯æŒ Baichuan2-7B
- \[2023/08\] TurboMind æ”¯æŒ flash-attention2
- \[2023/08\] TurboMind æ”¯æŒ Qwen-7Bï¼ŒåŠ¨æ€NTK-RoPEç¼©æ”¾ï¼ŒåŠ¨æ€logNç¼©æ”¾
- \[2023/08\] TurboMind æ”¯æŒ Windows (tp=1)
- \[2023/08\] TurboMind æ”¯æŒ 4-bit æ¨ç†ï¼Œé€Ÿåº¦æ˜¯ FP16 çš„ 2.4 å€ï¼Œæ˜¯ç›®å‰æœ€å¿«çš„å¼€æºå®ç°ğŸš€ã€‚éƒ¨ç½²æ–¹å¼è¯·çœ‹[è¿™é‡Œ](./docs/zh_cn/w4a16.md)
- \[2023/08\] LMDeploy å¼€é€šäº† [HuggingFace Hub](https://huggingface.co/lmdeploy) ï¼Œæä¾›å¼€ç®±å³ç”¨çš„ 4-bit æ¨¡å‹
- \[2023/08\] LMDeploy æ”¯æŒä½¿ç”¨ [AWQ](https://arxiv.org/abs/2306.00978) ç®—æ³•è¿›è¡Œ 4-bit é‡åŒ–
- \[2023/07\] TurboMind æ”¯æŒä½¿ç”¨ GQA çš„ Llama-2 70B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ Llama-2 7B/13B æ¨¡å‹
- \[2023/07\] TurboMind æ”¯æŒ InternLM çš„ Tensor Parallel æ¨ç†

______________________________________________________________________

# ç®€ä»‹

LMDeploy ç”± [MMDeploy](https://github.com/open-mmlab/mmdeploy) å’Œ [MMRazor](https://github.com/open-mmlab/mmrazor) å›¢é˜Ÿè”åˆå¼€å‘ï¼Œæ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡è§£å†³æ–¹æ¡ˆã€‚
è¿™ä¸ªå¼ºå¤§çš„å·¥å…·ç®±æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

- **é«˜æ•ˆæ¨ç†å¼•æ“ TurboMind**ï¼šå¼€å‘äº† Persistent Batch(å³ Continuous Batch)ï¼ŒBlocked K/V Cacheï¼ŒåŠ¨æ€æ‹†åˆ†å’Œèåˆï¼Œå¼ é‡å¹¶è¡Œï¼Œé«˜æ•ˆçš„è®¡ç®— kernelç­‰é‡è¦ç‰¹æ€§ï¼Œä¿éšœäº† LLMs æ¨ç†æ—¶çš„é«˜ååå’Œä½å»¶æ—¶ã€‚

- **æœ‰çŠ¶æ€æ¨ç†**ï¼šé€šè¿‡ç¼“å­˜å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­ attention çš„ k/vï¼Œè®°ä½å¯¹è¯å†å²ï¼Œä»è€Œé¿å…é‡å¤å¤„ç†å†å²ä¼šè¯ã€‚æ˜¾è‘—æå‡é•¿æ–‡æœ¬å¤šè½®å¯¹è¯åœºæ™¯ä¸­çš„æ•ˆç‡ã€‚

- **é‡åŒ–**ï¼šLMDeploy æ”¯æŒå¤šç§é‡åŒ–æ–¹å¼å’Œé«˜æ•ˆçš„é‡åŒ–æ¨¡å‹æ¨ç†ã€‚åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸Šï¼ŒéªŒè¯äº†é‡åŒ–çš„å¯é æ€§ã€‚

# æ€§èƒ½

LMDeploy TurboMind å¼•æ“æ‹¥æœ‰å“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨å„ç§è§„æ¨¡çš„æ¨¡å‹ä¸Šï¼Œæ¯ç§’å¤„ç†çš„è¯·æ±‚æ•°æ˜¯ vLLM çš„ 1.36 ~ 1.85 å€ã€‚åœ¨é™æ€æ¨ç†èƒ½åŠ›æ–¹é¢ï¼ŒTurboMind 4bit æ¨¡å‹æ¨ç†é€Ÿåº¦ï¼ˆout token/sï¼‰è¿œé«˜äº FP16/BF16 æ¨ç†ã€‚åœ¨å° batch æ—¶ï¼Œæé«˜åˆ° 2.4 å€ã€‚

![v0 1 0-benchmark](https://github.com/InternLM/lmdeploy/assets/4560679/f4d218f9-db3b-4ceb-ab50-97cb005b3ac9)

æ›´å¤šè®¾å¤‡ã€æ›´å¤šè®¡ç®—ç²¾åº¦ã€æ›´å¤šsettingä¸‹çš„çš„æ¨ç† benchmarkï¼Œè¯·å‚è€ƒä»¥ä¸‹é“¾æ¥ï¼š

- [A100](./docs/en/benchmark/a100_fp16.md)

# æ”¯æŒçš„æ¨¡å‹

`LMDeploy` æ”¯æŒ 2 ç§æ¨ç†å¼•æ“ï¼š `TurboMind` å’Œ `PyTorch`ï¼Œå®ƒä»¬ä¾§é‡ä¸åŒã€‚å‰è€…è¿½æ±‚æ¨ç†æ€§èƒ½çš„æè‡´ä¼˜åŒ–ï¼Œåè€…çº¯ç”¨pythonå¼€å‘ï¼Œç€é‡é™ä½å¼€å‘è€…çš„é—¨æ§›ã€‚

ä¸åŒçš„æ¨ç†å¼•æ“åœ¨æ”¯æŒçš„æ¨¡å‹ç±»åˆ«ã€è®¡ç®—ç²¾åº¦æ–¹é¢æœ‰æ‰€å·®åˆ«ã€‚ç”¨æˆ·å¯æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„ã€‚

## TurboMind æ”¯æŒçš„æ¨¡å‹

|        æ¨¡å‹        | æ¨¡å‹è§„æ¨¡ | FP16/BF16 | KV INT8 | W4A16 |
| :----------------: | :------: | :-------: | :-----: | :---: |
|       Llama        | 7B - 65B |    Yes    |   Yes   |  Yes  |
|       Llama2       | 7B - 70B |    Yes    |   Yes   |  Yes  |
|      InternLM      | 7B - 20B |    Yes    |   Yes   |  Yes  |
| InternLM-XComposer |    7B    |    Yes    |   Yes   |  Yes  |
|        QWen        | 7B - 72B |    Yes    |   Yes   |  Yes  |
|      QWen-VL       |    7B    |    Yes    |   Yes   |  Yes  |
|      Baichuan      |    7B    |    Yes    |   Yes   |  Yes  |
|     Baichuan2      |    7B    |    Yes    |   Yes   |  Yes  |
|     Code Llama     | 7B - 34B |    Yes    |   No    |  No   |

### PyTorch æ”¯æŒçš„æ¨¡å‹

|   æ¨¡å‹    | æ¨¡å‹è§„æ¨¡  | FP16/BF16 | KV INT8 | W8A8 |
| :-------: | :-------: | :-------: | :-----: | :--: |
|   Llama   | 7B - 65B  |    Yes    |   No    | Yes  |
|  Llama2   | 7B - 70B  |    Yes    |   No    | Yes  |
| InternLM  | 7B - 20B  |    Yes    |   No    | Yes  |
| Baichuan2 | 7B - 13B  |    Yes    |   No    | Yes  |
| ChatGLM2  |    6B     |    Yes    |   No    |  No  |
|  Falcon   | 7B - 180B |    Yes    |   No    |  No  |

# ç”¨æˆ·æ•™ç¨‹

è¯·é˜…è¯»[å¿«é€Ÿä¸Šæ‰‹](<>)ç« èŠ‚ï¼Œäº†è§£ LMDeploy çš„åŸºæœ¬ç”¨æ³•ã€‚

ä¸ºäº†å¸®åŠ©ç”¨æˆ·æ›´è¿›ä¸€æ­¥äº†è§£ LMDeployï¼Œæˆ‘ä»¬å‡†å¤‡äº†ç”¨æˆ·æŒ‡å—å’Œè¿›é˜¶æŒ‡å—ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„[æ–‡æ¡£](<>)ï¼š

- ç”¨æˆ·æŒ‡å—
  - æ¨ç†pipeline
  - [æ¨ç†å¼•æ“ - TurboMind](<>)
  - æ¨ç†å¼•æ“ - PyTorch
  - [æ¨ç†æœåŠ¡](<>)
  - [æ¨¡å‹é‡åŒ–](<>)
- è¿›é˜¶æŒ‡å—
  - å¢åŠ å¯¹è¯æ¨¡æ¿
  - æ”¯æŒæ–°æ¨¡å‹
  - gemm tuning
  - é•¿æ–‡æœ¬æ¨ç†

## è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ LMDeploy æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚è¯·å‚è€ƒ[è´¡çŒ®æŒ‡å—](.github/CONTRIBUTING.md)æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

## è‡´è°¢

- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [llm-awq](https://github.com/mit-han-lab/llm-awq)
- [vLLM](https://github.com/vllm-project/vllm)
- [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII)

## License

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache 2.0 å¼€æºè®¸å¯è¯](LICENSE)ã€‚
