model_path: /nvme/qa_test_models
dst_path: /nvme/qa_test_models/autotest_model
log_path: /nvme/qa_test_models/autotest_model/log
dataset_path: /nvme/qa_test_models/...dataset


tp_config:
    internlm-chat-20b: 2
    internlm2-chat-20b: 2
    Baichuan2-13B-Chat: 2
    Mixtral-8x7B-Instruct-v0.1: 2
    internlm2-20b: 2


turbomind_model:
    - llama-2-7b-chat
    - internlm2-chat-1_8b
    - internlm-chat-7b
    - internlm-chat-20b
    - internlm2-chat-7b
    - internlm2-chat-20b
    - Qwen-7B-Chat
    - Qwen-14B-Chat
    - llama2-chat-7b-w4
    - Baichuan2-7B-Chat
    - Yi-6B-Chat
    - internlm2-1_8b
    - internlm2-20b
    - CodeLlama-7b-Instruct-hf


pytorch_model:
    - llama-2-7b-chat
    - internlm-chat-7b
    - internlm-chat-20b
    - internlm2-chat-7b
    - internlm2-chat-20b
    - Baichuan2-7B-Chat
    - Baichuan2-13B-Chat
    - chatglm2-6b
    - falcon-7b
    - Yi-6B-Chat
    - internlm2-1_8b
    - internlm2-20b
    - Qwen1.5-7B-Chat
    - Mistral-7B-Instruct-v0.1
    - Mixtral-8x7B-Instruct-v0.1
    - gemma-7b-it
    - deepseek-moe-16b-chat


quatization_case_config:
    w4a16:
        - llama-2-7b-chat
        - internlm-chat-20b
        - Qwen-7B-Chat
        - Qwen-14B-Chat
        - internlm2-chat-20b
        - Baichuan2-7B-Chat
        - internlm2-20b
    kvint8: # more models are supported kvint8 quantization, but the chat response are not good, already removed
        - llama-2-7b-chat
        - internlm-chat-20b
        - internlm2-chat-20b
    kvint8_w4a16:
        - llama-2-7b-chat
        - internlm-chat-20b
        - internlm2-chat-20b
        - internlm2-20b
        - Qwen-7B-Chat
        - Qwen-14B-Chat
        - Baichuan2-7B-Chat
    w8a8:
        - llama-2-7b-chat
        - internlm-chat-20b
        - internlm2-chat-20b
        - internlm2-chat-7b
        - Yi-6B-Chat
        - internlm2-20b
