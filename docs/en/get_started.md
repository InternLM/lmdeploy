# Get Started

LMDeploy offers functionalities such as model quantization, offline batch inference, online serving, etc. Each function can be completed with just a few simple lines of code or commands.

## Installation

Install lmdeploy with pip ( python 3.8+) or [from source](./build.md)

```shell
pip install lmdeploy
```

## Offline batch inference

```shell
import lmdeploy
pipe = lmdeploy.pipeline("internlm/internlm-chat-7b", tp=1)
response = pipe(["Hi, pls intro yourself", "Shanghai is"])
print(response)
```

Tensor parallelism is supported, and can be invoked by setting the `tp` parameter.

<!-- For more information on inference pipeline parameters, please refer to [here(TODO)](<>). -->

## Serving

LMDeploy's `api_server` enables models to be easily packed into services with a single command. The provided RESTful APIs are compatible with OpenAI's interfaces. Below are an example of service startup:

```shell
lmdeploy serve api_server internlm/internlm-chat-7b --server-port 8080 --tp 1
```

After launching the server, you can communicate with server on terminal through `api_client`:

```shell
lmdeploy serve api_client http://0.0.0.0:8080
```

Besides the `api_client`, you can overview and try out `api_server` APIs online by swagger UI `http://0.0.0.0:8080`. And you can also read the API specification from [here](serving/restful_api.md).

## Quantization

### Weight INT4 Quantization

LMDeploy uses [AWQ](https://arxiv.org/abs/2306.00978) algorithm for model weight quantization

Using the following commands, you can quantize a LLM model into 4bit, and communicate with it with command line:

```shell
lmdeploy lite calibrate internlm/internlm-chat-7b --work-dir ./internlm-chat-7b-4bit
lmdeploy lite auto_awq internlm/internlm-chat-7b --work-dir ./internlm-chat-7b-4bit
lmdeploy chat turbomind ./internlm-chat-7b-4bit --model-format awq --group-size 128
```

LMDeploy 4bit inference supports the following NVIDIA GPU:

- Turing(sm75): 20 series, T4
- Ampere(sm80,sm86): 30 series, A10, A16, A30, A100
- Ada Lovelace(sm90): 40 series

Click [here](quantization/w4a16.md) to view more infermation about the inference of quantized models.

### KV Cache INT8 Quantization

Click [here](quantization/kv_int8.md) to view the usage method, implementation formula, and test results for kv int8.

### W8A8 Quantization

TODO
