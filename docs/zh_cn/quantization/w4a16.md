# INT4 模型量化和部署

LMDeploy 使用 AWQ 算法，实现模型 4bit 权重量化。推理引擎 TurboMind 提供了非常高效的 4bit 推理 cuda kernel，性能是 FP16 的 2.4 倍以上。它支持以下 NVIDIA 显卡：

- 图灵架构（sm75）：20系列、T4
- 安培架构（sm80,sm86）：30系列、A10、A16、A30、A100
- Ada Lovelace架构（sm90）：40 系列

在量化和部署之前，请确保安装了 lmdeploy

```shell
pip install lmdeploy[all]
```

本文由以下章节组成：

<!-- toc -->

- [模型量化](#模型量化)
- [模型评测](#模型评测)
  - [评测方法](#评测方法)
  - [评测结果](#评测结果)
- [模型推理](#模型推理)
- [推理性能](#推理性能)
- [推理服务](#推理服务)

<!-- tocstop -->

## 模型量化

仅需执行一条命令，就可以完成模型量化工作。量化结束后，权重文件存放在 `$WORK_DIR` 下。

```shell
lmdeploy lite auto_awq \
  --model $HF_MODEL \                # Model name or path, either model repo name on huggingface hub like 'internlm/internlm-chat-7b', or a model path in local host
  --calib_dataset 'c4' \             # Calibration dataset, supports c4, ptb, wikitext2, pileval
  --calib_samples 128 \              # Number of samples in the calibration set, if memory is insufficient, you can appropriately reduce this
  --calib_seqlen 2048 \              # Length of a single piece of text, if memory is insufficient, you can appropriately reduce this
  --w_bits 4 \                       # Bit number for weight quantization
  --w_group_size 128 \               # Group size for weight quantization statistics
  --work_dir $WORK_DIR \             # Folder storing Pytorch format quantization statistics parameters and post-quantization weight
```

绝大多数情况下，在执行上述命令时，可选参数可不用填写，使用默认的即可。比如量化 [internlm/internlm-chat-7b](https://huggingface.co/internlm/internlm-chat-7b) 模型，命令可以简化为：

```shell
lmdeploy lite auto_awq internlm/ianternlm-chat-7b --work_dir internlm-chat-7b-4bit
```

```{note}
我们建议 --work_dir 参数带有模型名字，就像上面的例子展示的那样。这样在推理时，就不用指定对话模板了。因为推理接口会以模糊搜索方式，选出和 --work-dir 近似的对话模板
```

量化后的模型，可以用一些工具快速验证对话效果。

比如，直接在控制台和模型对话，

```shell
lmdeploy chat turbomind ./internlm-chat-7b-4bit
```

或者，启动gradio服务，

```shell
lmdeploy serve gradio ./internlm-chat-7b-4bit --server_name {ip_addr} --server_port {port}
```

然后，在浏览器中打开 http://{ip_addr}:{port}，即可在线对话

## 模型评测

我们使用 [OpenCompass](<>) 评测量化模型在各个维度上的能力

### 评测方法

TBD

### 评测结果

TBD

## 模型推理

量化后的模型，通过以下几行简单的代码，可以实现离线推理：

```python
import lmdeploy
pipe = lmdeploy.pipeline("./internlm-chat-7b-4bit")
response = pipe(["Hi, pls intro yourself", "Shanghai is"])
print(response)
```

关于 pipeline 的详细介绍，请参考[这里](../inference/pipeline.md)

除了推理本地量化模型外，LMDeploy 还支持直接推理 huggingface hub 上的通过 AWQ 量化的 4bit 权重模型，比如 [lmdeploy 空间](https://huggingface.co/lmdeploy)和 [TheBloke 空间](https://huggingface.co/TheBloke)下的模型。

```python
# 推理 lmdeploy 空间下的模型
import lmdeploy
pipe = lmdeploy.pipeline("lmdeploy/llama2-chat-70b-4bit", tp=4)
response = pipe(["Hi, pls intro yourself", "Shanghai is"])
print(response)

# 推理 TheBloke 空间下的模型（试试codellama行不行）
import lmdeploy
pipe = lmdeploy.pipeline("TheBloke/LLaMA2-13B-Tiefighter-AWQ")
response = pipe(["Hi, pls intro yourself", "Shanghai is"])
print(response)
```

## 推理性能

我们在 NVIDIA GeForce RTX 4090 上使用 [profile_generation.py](https://github.com/InternLM/lmdeploy/blob/main/benchmark/profile_generation.py)，分别测试了 4-bit Llama-2-7B-chat 和 Llama-2-13B-chat 模型的 token 生成速度。测试配置为 batch size = 1，(prompt_tokens, completion_tokens) = (1, 512)

| model            | llm-awq | mlc-llm | turbomind |
| ---------------- | ------- | ------- | --------- |
| Llama-2-7B-chat  | 112.9   | 159.4   | 206.4     |
| Llama-2-13B-chat | N/A     | 90.7    | 115.8     |

上述两个模型的16bit 和 4bit 权重，分别使用 turbomind 推理时，各自在context size 为 2048 和 4096 配置下，所占的显存对比如下：

| model            | 16bit(2048) | 4bit(2048) | 16bit(4096) | 4bit(4096) |
| ---------------- | ----------- | ---------- | ----------- | ---------- |
| Llama-2-7B-chat  | 15.1        | 6.3        | 16.2        | 7.5        |
| Llama-2-13B-chat | OOM         | 10.3       | OOM         | 12.0       |

`profile_generation.py` 可以用来进行性能测试

```shell
pip install nvidia-ml-py
```

```shell
python benchmark/profile_generation.py \
 --model-path ./workspace \
 --concurrency 1 8 --prompt-tokens 1 512 --completion-tokens 2048 512
```

## 推理服务
